"""
AgenticFlow MCP Server â€” OpenClaw í†µí•©ìš© ìƒì£¼í˜• ì„œë²„
====================================================
FastMCP ê¸°ë°˜ì˜ MCP(Model Context Protocol) ì„œë²„ì…ë‹ˆë‹¤.
OpenClawê°€ í‘œì¤€ í”„ë¡œí† ì½œë¡œ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

í•µì‹¬ íŠ¹ì§•:
- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ warm-up (ì½œë“œ ìŠ¤íƒ€íŠ¸ ì œê±°)
- ì„¸ì…˜ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ (ì‚¬ê³  ê³¼ì • ìœ ì§€)
- EventBus ì—°ë™ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
- ë¹„ë™ê¸° ì²˜ë¦¬ (ì„œë²„ ë…¼ë¸”ë¡œí‚¹)

ì‹¤í–‰:
    python server.py                    # stdio ëª¨ë“œ (OpenClaw ì—°ë™)
    python server.py --transport sse    # SSE ëª¨ë“œ (ë””ë²„ê¹…ìš©)
"""

from __future__ import annotations

import asyncio
import logging
import os
import sys
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any

import yaml
from dotenv import load_dotenv

from core.engine_mlx import MLXEngine, MLXConfig, EngineBackend
from core.event_bus import EventBus, Event, EventType
from utils.hardware_probe import HardwareProbe

# â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(name)-20s â”‚ %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("mcp-server")

# â”€â”€ ì„¸ì…˜ ë°ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class AgentSession:
    """ì—ì´ì „íŠ¸ ì„¸ì…˜ ìƒíƒœ."""
    session_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    topic: str = ""
    mode: str = "research"
    status: str = "idle"
    thought_trace: list[dict[str, Any]] = field(default_factory=list)
    result: str = ""
    created_at: str = field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )
    step_count: int = 0

    def add_thought(self, step: str, content: str) -> None:
        """ì‚¬ê³  ê³¼ì •ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
        self.thought_trace.append({
            "step": step,
            "content": content,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        self.step_count += 1

    def to_dict(self) -> dict[str, Any]:
        """ì§ë ¬í™”ìš© ë”•ì…”ë„ˆë¦¬."""
        return {
            "session_id": self.session_id,
            "topic": self.topic,
            "mode": self.mode,
            "status": self.status,
            "step_count": self.step_count,
            "created_at": self.created_at,
        }


# â”€â”€ ì„œë²„ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ì „ì—­ ìƒíƒœ
engine: MLXEngine | None = None
event_bus: EventBus | None = None
sessions: dict[str, AgentSession] = {}
probe = HardwareProbe()


def _load_mlx_config() -> MLXConfig:
    """config/m4_32gb.yamlì—ì„œ MLX ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    config_path = os.path.join(
        os.path.dirname(__file__), "config", "m4_32gb.yaml"
    )
    if os.path.exists(config_path):
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
            mlx_data = data.get("mlx", {})
            return MLXConfig.from_dict(mlx_data)
        except Exception as e:
            logger.warning(f"âš ï¸ Config load failed, using defaults: {e}")
    return MLXConfig()


# â”€â”€ Lifecycle (lifespan context manager) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from contextlib import asynccontextmanager
from collections.abc import AsyncIterator


@asynccontextmanager
async def server_lifespan(app: "FastMCP") -> AsyncIterator[None]:  # type: ignore[name-defined]
    """ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ."""
    global engine, event_bus

    logger.info("ğŸš€ AgenticFlow MCP Server starting...")

    # í•˜ë“œì›¨ì–´ ì§„ë‹¨
    summary = probe.get_summary()
    logger.info(
        f"ğŸ” Hardware: {summary['chip']['brand']} | "
        f"RAM: {summary['memory']['total_gb']}GB | "
        f"GPU: {summary['chip']['gpu_cores']} cores"
    )

    # ì´ë²¤íŠ¸ ë²„ìŠ¤ ì´ˆê¸°í™”
    EventBus.reset()
    event_bus = EventBus()
    await event_bus.start()

    # MLX ì—”ì§„ ì´ˆê¸°í™”
    config = _load_mlx_config()
    engine = MLXEngine(
        config=config,
        litellm_base_url=os.getenv(
            "LITELLM_BASE_URL", "http://localhost:4000"
        ),
    )
    await engine.load()

    logger.info(
        f"âœ… MCP Server ready | Backend: {engine.backend.value} | "
        f"Model: {config.main_model}"
    )

    yield  # ì„œë²„ ì‹¤í–‰ ì¤‘

    # Shutdown
    if engine:
        await engine.unload()
    if event_bus:
        await event_bus.stop()
    logger.info("ğŸ‘‹ MCP Server stopped")


# â”€â”€ FastMCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    from fastmcp import FastMCP  # type: ignore[import-untyped]
    mcp = FastMCP(
        "AgenticFlow-M4",
        instructions=(
            "AgenticFlowëŠ” Mac Mini M4ì—ì„œ êµ¬ë™ë˜ëŠ” ê³ ì„±ëŠ¥ ë¡œì»¬ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. "
            "ì‹¬ì¸µ ë¶„ì„, ì½”ë“œ ë¦¬íŒ©í† ë§, ë³µì¡í•œ ê³„íš ìˆ˜ë¦½ì— ì í•©í•©ë‹ˆë‹¤. "
            "ë‹¨ìˆœ ì§ˆë¬¸ì—ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
        ),
        lifespan=server_lifespan,
    )
    _MCP_AVAILABLE = True
except ImportError:
    mcp = None  # type: ignore[assignment]
    _MCP_AVAILABLE = False
    logger.warning("âš ï¸ fastmcp not installed â€” MCP server disabled")


# â”€â”€ MCP Tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@mcp.tool()
async def run_flow(
    topic: str,
    mode: str = "research",
    max_tokens: int = 2048,
) -> str:
    """ì—ì´ì „íŠ¸ í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        topic: ì‘ì—… ì£¼ì œ ë˜ëŠ” ì§ˆë¬¸
        mode: ì‹¤í–‰ ëª¨ë“œ (research | code | plan | analyze)
        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜

    Returns:
        ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µ
    """
    if not engine:
        return "[ERROR] Engine not initialized"

    # ì„¸ì…˜ ìƒì„±
    session = AgentSession(topic=topic, mode=mode)
    sessions[session.session_id] = session
    session.status = "running"

    # ì‚¬ê³  ê³¼ì • ê¸°ë¡
    session.add_thought("init", f"Starting {mode} flow: {topic}")

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompts: dict[str, str] = {
        "research": (
            "You are a senior research analyst. Analyze the given topic "
            "thoroughly with structured insights, pros/cons, and "
            "actionable recommendations."
        ),
        "code": (
            "You are an expert software engineer specializing in "
            "Python and TypeScript. Write clean, well-documented, "
            "production-ready code with proper error handling."
        ),
        "plan": (
            "You are a strategic project planner. Create detailed, "
            "phased implementation plans with timelines, dependencies, "
            "and risk assessments."
        ),
        "analyze": (
            "You are a systems analyst. Perform deep analysis of the "
            "given topic covering architecture, performance, security, "
            "and scalability aspects."
        ),
    }
    system_prompt = system_prompts.get(
        mode, system_prompts["research"]
    )

    session.add_thought("routing", f"Using {mode} system prompt")

    # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬
    if probe.should_fallback():
        session.add_thought(
            "warning",
            "Memory pressure detected â€” response may be shorter"
        )
        max_tokens = min(max_tokens, 1024)

    # ì¶”ë¡  ì‹¤í–‰
    session.add_thought("inference", "Generating response...")

    try:
        result = await engine.generate(
            prompt=topic,
            max_tokens=max_tokens,
            system_prompt=system_prompt,
        )

        session.result = result.text
        session.status = "completed"
        session.add_thought(
            "done",
            f"Generated {result.tokens_generated} tokens "
            f"({result.tokens_per_second} tok/s, "
            f"{result.elapsed_ms:.0f}ms) "
            f"[{result.backend.value}]"
        )

        # ì´ë²¤íŠ¸ ë°œí–‰
        if event_bus:
            await event_bus.publish(Event(
                type=EventType.AGENT_RESPONSE,
                source="mcp-server",
                payload={
                    "session_id": session.session_id,
                    "tokens": result.tokens_generated,
                    "tps": result.tokens_per_second,
                    "backend": result.backend.value,
                },
            ))

        return result.text

    except Exception as e:
        session.status = "failed"
        session.add_thought("error", str(e))
        logger.error(f"âŒ Flow execution failed: {e}")
        return f"[ERROR] Flow execution failed: {e}"


@mcp.tool()
async def get_status(session_id: str) -> str:
    """ì„¸ì…˜ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        session_id: ì„¸ì…˜ UUID

    Returns:
        ì„¸ì…˜ ìƒíƒœ JSON ë¬¸ìì—´
    """
    import json

    session = sessions.get(session_id)
    if not session:
        return json.dumps({"error": f"Session {session_id} not found"})

    return json.dumps(session.to_dict(), indent=2, ensure_ascii=False)


@mcp.tool()
async def get_thought_trace(
    session_id: str,
    limit: int = 20,
) -> str:
    """ì—ì´ì „íŠ¸ì˜ ì‚¬ê³  ê³¼ì •(Thought Trace)ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        session_id: ì„¸ì…˜ UUID
        limit: ìµœëŒ€ ë°˜í™˜ í•­ëª© ìˆ˜

    Returns:
        ì‚¬ê³  ê³¼ì • ë¡œê·¸
    """
    import json

    session = sessions.get(session_id)
    if not session:
        return json.dumps({"error": f"Session {session_id} not found"})

    trace = session.thought_trace[-limit:]
    return json.dumps(trace, indent=2, ensure_ascii=False)


@mcp.tool()
async def list_sessions() -> str:
    """í™œì„± ì„¸ì…˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        ì„¸ì…˜ ëª©ë¡ JSON ë¬¸ìì—´
    """
    import json

    session_list = [s.to_dict() for s in sessions.values()]
    return json.dumps(session_list, indent=2, ensure_ascii=False)


@mcp.tool()
async def get_hardware_info() -> str:
    """í˜„ì¬ í•˜ë“œì›¨ì–´ ì •ë³´ ë° ëª¨ë¸ ì¶”ì²œì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        í•˜ë“œì›¨ì–´ ì •ë³´ JSON ë¬¸ìì—´
    """
    import json

    summary = probe.get_summary()
    if engine:
        summary["engine"] = engine.get_stats()

    return json.dumps(summary, indent=2, ensure_ascii=False)


@mcp.tool()
async def clear_session(session_id: str) -> str:
    """ì„¸ì…˜ì„ ì‚­ì œí•©ë‹ˆë‹¤.

    Args:
        session_id: ì‚­ì œí•  ì„¸ì…˜ UUID

    Returns:
        ì‚­ì œ ê²°ê³¼ ë©”ì‹œì§€
    """
    if session_id in sessions:
        del sessions[session_id]
        return f"Session {session_id} cleared"
    return f"Session {session_id} not found"


# â”€â”€ MCP Resources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@mcp.resource("agentic://status")  # type: ignore[misc]
async def resource_status() -> str:
    """ì„œë²„ ìƒíƒœ ë¦¬ì†ŒìŠ¤."""
    import json

    status: dict[str, Any] = {
        "server": "AgenticFlow-M4",
        "active_sessions": len(sessions),
        "engine_loaded": engine.is_loaded if engine else False,
        "backend": engine.backend.value if engine else "none",
    }
    return json.dumps(status, indent=2)


# â”€â”€ Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="AgenticFlow MCP Server (M4 Optimized)"
    )
    parser.add_argument(
        "--transport",
        choices=["stdio", "sse"],
        default="stdio",
        help="MCP transport (default: stdio for OpenClaw)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8765,
        help="SSE port (only for sse transport)",
    )
    args = parser.parse_args()

    logger.info(
        f"ğŸ Starting MCP Server (transport: {args.transport})"
    )

    if args.transport == "sse":
        mcp.run(transport="sse", port=args.port)
    else:
        mcp.run(transport="stdio")
