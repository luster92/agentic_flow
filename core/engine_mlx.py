"""
MLXEngine â€” Apple Silicon ì „ìš© ì¶”ë¡  ì—”ì§„
==========================================
Apple MLX í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ M4 GPUì—ì„œ ì§ì ‘ ì¶”ë¡ í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- 4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë”© (Qwen2.5-32B-Instruct-4bit)
- íˆ¬ê¸°ì  ë””ì½”ë”© (Speculative Decoding) â€” ë“œë˜í”„íŠ¸ ëª¨ë¸ë¡œ 2ë°° ì†ë„
- KV Cache ì–‘ìí™” (4-bit) â€” ê¸´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ OOM ë°©ì§€
- ë™ì  ë©”ëª¨ë¦¬ ê´€ë¦¬ â€” ê°€ìš© ë©”ëª¨ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì „í™˜

MLX ë¯¸ì„¤ì¹˜ í™˜ê²½ì—ì„œëŠ” LiteLLM fallbackìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import asyncio
import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, AsyncIterator

logger = logging.getLogger(__name__)

# â”€â”€ MLX import guard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_MLX_AVAILABLE = False
_mlx = None
_mlx_lm = None
_mlx_lm_generate = None

try:
    import mlx.core as _mlx  # type: ignore[import-untyped]
    import mlx_lm  # type: ignore[import-untyped]
    from mlx_lm import load as _mlx_load  # type: ignore[import-untyped]
    from mlx_lm import generate as _mlx_generate  # type: ignore[import-untyped]
    _MLX_AVAILABLE = True
    logger.info("âœ… MLX backend available (Apple Silicon)")
except ImportError:
    logger.info(
        "âš ï¸ MLX not available â€” using LiteLLM fallback. "
        "Install with: pip install mlx mlx-lm"
    )


# â”€â”€ Enums & Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EngineBackend(str, Enum):
    """ì¶”ë¡  ë°±ì—”ë“œ."""
    MLX = "mlx"
    LITELLM = "litellm"


@dataclass
class MLXConfig:
    """MLX ì—”ì§„ ì„¤ì •.

    config/m4_32gb.yamlì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©.
    """
    main_model: str = "mlx-community/Qwen2.5-32B-Instruct-4bit"
    draft_model: str = "mlx-community/Qwen2.5-0.5B-Instruct-4bit"
    speculative_decoding: bool = True
    kv_cache_bits: int = 4
    max_context_length: int = 8192
    max_tokens: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.1
    # ë©”ëª¨ë¦¬ ì•ˆì „ ë§ˆì§„
    model_budget_gb: float = 22.0
    fallback_threshold_gb: float = 4.0

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> MLXConfig:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ MLXConfig ìƒì„±."""
        return cls(**{
            k: v for k, v in data.items()
            if k in cls.__dataclass_fields__
        })


# â”€â”€ Generation Result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class GenerationResult:
    """í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼."""
    text: str
    tokens_generated: int = 0
    tokens_per_second: float = 0.0
    elapsed_ms: float = 0.0
    backend: EngineBackend = EngineBackend.MLX
    draft_acceptance_rate: float = 0.0
    metadata: dict[str, Any] = field(default_factory=dict)


# â”€â”€ MLX Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MLXEngine:
    """Apple Silicon ì „ìš© MLX ì¶”ë¡  ì—”ì§„.

    M4 GPUì—ì„œ ì§ì ‘ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    MLXê°€ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” LiteLLM fallbackìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

    Usage:
        engine = MLXEngine()
        await engine.load()
        result = await engine.generate("Hello, world!")
        await engine.unload()
    """

    def __init__(
        self,
        config: MLXConfig | None = None,
        litellm_base_url: str = "http://localhost:4000",
    ) -> None:
        self.config = config or MLXConfig()
        self._litellm_base_url = litellm_base_url

        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        self._model: Any = None
        self._tokenizer: Any = None
        self._draft_model: Any = None
        self._draft_tokenizer: Any = None

        # ìƒíƒœ ì¶”ì 
        self._loaded: bool = False
        self._backend: EngineBackend = (
            EngineBackend.MLX if _MLX_AVAILABLE
            else EngineBackend.LITELLM
        )
        self._total_tokens: int = 0
        self._total_requests: int = 0

    # â”€â”€ ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def load(self) -> bool:
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.

        Returns:
            True if loaded successfully
        """
        if self._loaded:
            logger.info("âš¡ Models already loaded, skipping")
            return True

        if not _MLX_AVAILABLE:
            logger.info(
                "ğŸ”„ MLX unavailable, using LiteLLM fallback "
                f"(proxy: {self._litellm_base_url})"
            )
            self._backend = EngineBackend.LITELLM
            self._loaded = True
            return True

        try:
            return await asyncio.to_thread(self._load_sync)
        except Exception as e:
            logger.error(f"âŒ MLX model loading failed: {e}")
            logger.info("ğŸ”„ Falling back to LiteLLM")
            self._backend = EngineBackend.LITELLM
            self._loaded = True
            return True

    def _load_sync(self) -> bool:
        """ë™ê¸° ëª¨ë¸ ë¡œë”© (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)."""
        start = time.monotonic()

        # ë©”ì¸ ëª¨ë¸ ë¡œë“œ
        logger.info(
            f"ğŸ§  Loading main model: {self.config.main_model}"
        )
        self._model, self._tokenizer = _mlx_load(
            self.config.main_model,
        )
        elapsed_main = time.monotonic() - start
        logger.info(
            f"âœ… Main model loaded in {elapsed_main:.1f}s"
        )

        # ë“œë˜í”„íŠ¸ ëª¨ë¸ ë¡œë“œ (íˆ¬ê¸°ì  ë””ì½”ë”©ìš©)
        if self.config.speculative_decoding:
            logger.info(
                f"ğŸƒ Loading draft model: {self.config.draft_model}"
            )
            draft_start = time.monotonic()
            self._draft_model, self._draft_tokenizer = _mlx_load(
                self.config.draft_model,
            )
            elapsed_draft = time.monotonic() - draft_start
            logger.info(
                f"âœ… Draft model loaded in {elapsed_draft:.1f}s"
            )

        self._loaded = True
        self._backend = EngineBackend.MLX
        total = time.monotonic() - start
        logger.info(
            f"ğŸš€ MLX Engine ready! "
            f"Total load time: {total:.1f}s | "
            f"Backend: {self._backend.value}"
        )
        return True

    # â”€â”€ ëª¨ë¸ ì–¸ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def unload(self) -> None:
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤."""
        if not self._loaded:
            return

        if self._backend == EngineBackend.MLX and _MLX_AVAILABLE:
            self._model = None
            self._tokenizer = None
            self._draft_model = None
            self._draft_tokenizer = None

            # GPU ë©”ëª¨ë¦¬ ëª…ì‹œì  í•´ì œ
            try:
                _mlx.metal.clear_cache()  # type: ignore[union-attr]
                logger.info("ğŸ§¹ GPU cache cleared")
            except Exception as e:
                logger.warning(f"âš ï¸ Cache clear failed: {e}")

        self._loaded = False
        logger.info("ğŸ”Œ MLX Engine unloaded")

    # â”€â”€ í…ìŠ¤íŠ¸ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def generate(
        self,
        prompt: str,
        *,
        max_tokens: int | None = None,
        temperature: float | None = None,
        system_prompt: str | None = None,
    ) -> GenerationResult:
        """í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            prompt: ì‚¬ìš©ì ì…ë ¥
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

        Returns:
            GenerationResult
        """
        if not self._loaded:
            await self.load()

        max_tok = max_tokens or self.config.max_tokens
        temp = temperature or self.config.temperature

        if self._backend == EngineBackend.LITELLM:
            return await self._generate_litellm(
                prompt, max_tok, temp, system_prompt
            )

        return await self._generate_mlx(
            prompt, max_tok, temp, system_prompt
        )

    async def _generate_mlx(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str | None,
    ) -> GenerationResult:
        """MLX ë°±ì—”ë“œë¡œ í…ìŠ¤íŠ¸ ìƒì„±."""
        start = time.monotonic()

        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        messages: list[dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        if hasattr(self._tokenizer, "apply_chat_template"):
            formatted = self._tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
        else:
            formatted = prompt

        def _run() -> tuple[str, dict[str, Any]]:
            gen_kwargs: dict[str, Any] = {
                "temp": temperature,
                "max_tokens": max_tokens,
                "repetition_penalty": self.config.repetition_penalty,
            }

            # íˆ¬ê¸°ì  ë””ì½”ë”©
            if (
                self.config.speculative_decoding
                and self._draft_model is not None
            ):
                gen_kwargs["draft_model"] = self._draft_model

            result_text = _mlx_generate(
                self._model,
                self._tokenizer,
                prompt=formatted,
                **gen_kwargs,
            )

            return result_text, gen_kwargs

        text, kwargs = await asyncio.to_thread(_run)

        elapsed = (time.monotonic() - start) * 1000
        # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
        token_count = len(text.split()) * 1.3  # ê·¼ì‚¬ì¹˜
        tps = (token_count / (elapsed / 1000)) if elapsed > 0 else 0

        self._total_tokens += int(token_count)
        self._total_requests += 1

        # GPU ìºì‹œ ì •ë¦¬ (ê¸´ ìƒì„± í›„)
        if token_count > 500 and _MLX_AVAILABLE:
            try:
                _mlx.metal.clear_cache()  # type: ignore[union-attr]
            except Exception:
                pass

        return GenerationResult(
            text=text,
            tokens_generated=int(token_count),
            tokens_per_second=round(tps, 1),
            elapsed_ms=round(elapsed, 1),
            backend=EngineBackend.MLX,
            metadata={
                "speculative": self.config.speculative_decoding,
                "model": self.config.main_model,
            },
        )

    async def _generate_litellm(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str | None,
    ) -> GenerationResult:
        """LiteLLM fallbackìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±."""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            return GenerationResult(
                text="[ERROR] OpenAI client not installed",
                backend=EngineBackend.LITELLM,
            )

        start = time.monotonic()
        client = AsyncOpenAI(
            base_url=self._litellm_base_url,
            api_key="not-needed",
        )

        messages: list[dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            response = await client.chat.completions.create(
                model="local-worker",
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            text = response.choices[0].message.content or ""
            elapsed = (time.monotonic() - start) * 1000
            tokens = response.usage.completion_tokens if response.usage else 0

            self._total_tokens += tokens
            self._total_requests += 1

            return GenerationResult(
                text=text,
                tokens_generated=tokens,
                tokens_per_second=round(
                    tokens / (elapsed / 1000), 1
                ) if elapsed > 0 else 0,
                elapsed_ms=round(elapsed, 1),
                backend=EngineBackend.LITELLM,
                metadata={"model": "local-worker"},
            )
        except Exception as e:
            logger.error(f"âŒ LiteLLM generation failed: {e}")
            return GenerationResult(
                text=f"[ERROR] LiteLLM generation failed: {e}",
                backend=EngineBackend.LITELLM,
            )

    # â”€â”€ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def generate_stream(
        self,
        prompt: str,
        *,
        max_tokens: int | None = None,
        temperature: float | None = None,
        system_prompt: str | None = None,
    ) -> AsyncIterator[str]:
        """í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±.

        Yields:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬
        """
        if not self._loaded:
            await self.load()

        if self._backend == EngineBackend.LITELLM:
            async for chunk in self._stream_litellm(
                prompt,
                max_tokens or self.config.max_tokens,
                temperature or self.config.temperature,
                system_prompt,
            ):
                yield chunk
        else:
            # MLXëŠ” í˜„ì¬ ë™ê¸° ìƒì„±ë§Œ ì§€ì› â†’ ì „ì²´ ê²°ê³¼ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            result = await self.generate(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                system_prompt=system_prompt,
            )
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            sentences = result.text.split(". ")
            for i, sentence in enumerate(sentences):
                chunk = sentence + (". " if i < len(sentences) - 1 else "")
                yield chunk
                await asyncio.sleep(0.01)

    async def _stream_litellm(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str | None,
    ) -> AsyncIterator[str]:
        """LiteLLM ìŠ¤íŠ¸ë¦¬ë°."""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            yield "[ERROR] OpenAI client not installed"
            return

        client = AsyncOpenAI(
            base_url=self._litellm_base_url,
            api_key="not-needed",
        )

        messages: list[dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            stream = await client.chat.completions.create(
                model="local-worker",
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
            )
            async for chunk in stream:
                content = chunk.choices[0].delta.content or ""
                if content:
                    yield content
        except Exception as e:
            yield f"[ERROR] Streaming failed: {e}"

    # â”€â”€ ìƒíƒœ ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë”© ì—¬ë¶€."""
        return self._loaded

    @property
    def backend(self) -> EngineBackend:
        """í˜„ì¬ ì¶”ë¡  ë°±ì—”ë“œ."""
        return self._backend

    @property
    def is_mlx(self) -> bool:
        """MLX ë°±ì—”ë“œ ì‚¬ìš© ì—¬ë¶€."""
        return self._backend == EngineBackend.MLX

    def get_stats(self) -> dict[str, Any]:
        """ì—”ì§„ í†µê³„ ë°˜í™˜."""
        return {
            "loaded": self._loaded,
            "backend": self._backend.value,
            "mlx_available": _MLX_AVAILABLE,
            "main_model": self.config.main_model,
            "draft_model": (
                self.config.draft_model
                if self.config.speculative_decoding
                else None
            ),
            "speculative_decoding": self.config.speculative_decoding,
            "kv_cache_bits": self.config.kv_cache_bits,
            "total_tokens": self._total_tokens,
            "total_requests": self._total_requests,
        }
