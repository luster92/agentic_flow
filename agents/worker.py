"""
Worker Agent - Qwen 2.5 Coder 32B (Int4)
=========================================
í•µì‹¬ ì½”ë“œ êµ¬í˜„ ë° ëª¨ë“ˆ ê°œë°œ ì—­í• :
- ë³µì¡í•œ ë…¼ë¦¬ì™€ ì½”ë”©ì„ ì§ì ‘ ìˆ˜í–‰
- ë‹¨ìˆœ ë°˜ë³µ ì‘ì—…ì€ Helperì—ê²Œ ìœ„ì„ (call_helper)
- [NEW] ë„êµ¬ ì‚¬ìš© (Tools): FileRead, ListDir ë“± ì™¸ë¶€ ìƒí˜¸ì‘ìš©
- [NEW] ì¥ê¸° ê¸°ì–µ (Memory): ê³¼ê±°ì˜ ê²½í—˜(VectorDB)ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œ í•´ê²°
- [NEW] MCP í™•ì¥ (Model Context Protocol): ì™¸ë¶€ ë„êµ¬ ë™ì  ë¡œë“œ
- Helper ì‹¤íŒ¨ ì‹œ ì§ì ‘ ì²˜ë¦¬ (Do It Yourself Fallback)
- ë„ì €íˆ í•´ê²° ë¶ˆê°€ëŠ¥í•œ ë‚œì œì—ë§Œ [ESCALATE] ì¶œë ¥

ê²€ì¦ íŒŒì´í”„ë¼ì¸ (AI Dunning-Kruger ë°©ì–´):
1. ê²°ì •ë¡ ì  ê²€ì¦ (ast.parse) â€” ì½”ë“œ ë¬¸ë²• ê¸°ê³„ì  ê²€ì¦
2. Critic Agent â€” ê¹Œì¹ í•œ ì½”ë“œ ë¦¬ë·°ì–´ê°€ [PASS]/[REJECT] íŒì •
   â†’ ë©€í‹°í„´ Critic: REJECT ì‹œ suggestions ê¸°ë°˜ ì¬ìƒì„± (ìµœëŒ€ 2íšŒ)
3. Self-Reflection â€” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‚´ ìê¸° ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
"""

import asyncio
import json
import logging
from openai import AsyncOpenAI

from agents.helper import ask_helper_safe
from agents.critic import critique
from utils.validator import validate_response, format_error_feedback
from utils.introspector import generate_context as generate_knowledge_context
from utils.knowledge_updater import record_learning
from utils.tools import AVAILABLE_TOOLS
from utils.memory import global_memory
from utils.mcp_client import global_mcp_manager
from core.observability import TokenUsageTracker

logger = logging.getLogger(__name__)

# â”€â”€ Worker ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Self-Reflection í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WORKER_SYSTEM_PROMPT = """ë„ˆëŠ” ë…¸ë ¨í•œ ìˆ˜ì„ ê°œë°œì(Worker)ë‹¤.

1. ë³µì¡í•œ ë…¼ë¦¬ì™€ ì½”ë”©ì€ ë„¤ê°€ ì§ì ‘ ìˆ˜í–‰í•´ë¼.
2. ë‹¨ìˆœ ë°˜ë³µ ì‘ì—…(ì£¼ì„, í¬ë§·íŒ…)ì€ 'helper_tool'ì„ ì‚¬ìš©í•´ë¼.
3. ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì œê³µëœ ë„êµ¬(Tools)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì‹œìŠ¤í…œ ë“±ì„ íƒìƒ‰í•´ë¼.
4. ë§Œì•½ ë„ì €íˆ í•´ê²° ë¶ˆê°€ëŠ¥í•œ ë‚œì œì— ë´‰ì°©í•˜ë©´ '[ESCALATE]'ë¼ê³  ì¶œë ¥í•´ë¼.
5. ë‹¨, Helperì˜ ì‹¤íŒ¨ ë•Œë¬¸ì— ì—ìŠ¤ì»¬ë ˆì´ì…˜í•˜ì§€ ë§ˆë¼. Helperê°€ ëª»í•˜ë©´ ë„¤ê°€ ì§ì ‘ ì²˜ë¦¬í•´ë¼.

## Self-Reflection (ìê¸° ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸)
ë‹µë³€ì„ ìµœì¢… ì¶œë ¥í•˜ê¸° ì „ì— ë‹¤ìŒì„ ìŠ¤ìŠ¤ë¡œ ì ê²€í•´ë¼:
1. ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì„ 100% ì¶©ì¡±í–ˆëŠ”ê°€?
2. ì½”ë“œê°€ ì‹¤ì œë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ìƒíƒœì¸ê°€? (import ëˆ„ë½, ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜ ì—†ëŠ”ê°€?)
3. ì—£ì§€ ì¼€ì´ìŠ¤(Edge Case)ëŠ” ê³ ë ¤í–ˆëŠ”ê°€?
4. ë³€ìˆ˜ëª…ê³¼ í•¨ìˆ˜ëª…ì´ ëª…í™•í•œê°€?

âš ï¸ ë§Œì•½ 1%ë¼ë„ í™•ì‹ ì´ ì—†ë‹¤ë©´, ì–µì§€ë¡œ ë‹µì„ ë§Œë“¤ì§€ ë§ê³  '[ESCALATE]'ë¥¼ ì¶œë ¥í•´ë¼.
ì‘ë‹µí•  ë•ŒëŠ” í•­ìƒ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì½”ë“œë¥¼ ì œê³µí•´ë¼."""

# â”€â”€ Helper ìœ„ì„ ê°€ëŠ¥í•œ ì‘ì—… í‚¤ì›Œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HELPER_DELEGATABLE_KEYWORDS = [
    "ì£¼ì„ ì¶”ê°€", "add comments", "comment",
    "í¬ë§·íŒ…", "formatting", "format",
    "ë²ˆì—­", "translate", "translation",
    "docstring", "ë…ìŠ¤íŠ¸ë§",
    "íƒ€ì… íŒíŠ¸", "type hint",
    "ë¦°íŠ¸", "lint", "linting",
]

# â”€â”€ ê²€ì¦ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_VALIDATION_RETRIES = 2  # ë¬¸ë²• ì˜¤ë¥˜ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
MAX_CRITIC_ROUNDS = 2       # Critic ë©€í‹°í„´ ìµœëŒ€ ë¼ìš´ë“œ
MAX_TOOL_STEPS = 5          # ë„êµ¬ ì—°ì† í˜¸ì¶œ ìµœëŒ€ íšŸìˆ˜ (Re-act Loop Limit)


class Worker:
    """
    Qwen 2.5 Coder 32B ê¸°ë°˜ í•µì‹¬ ì›Œì»¤.
    ë³µì¡í•œ ì½”ë“œ êµ¬í˜„ì„ ìˆ˜í–‰í•˜ë©°, ë‹¨ìˆœ ì‘ì—…ì€ Helperì— ìœ„ì„í•©ë‹ˆë‹¤.
    3ë‹¨ê³„ ê²€ì¦ íŒŒì´í”„ë¼ì¸(ê²°ì •ë¡ ì  ê²€ì¦ + Critic + Self-Reflection)ì„ í†µí•´
    'AI Dunning-Kruger Effect'ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        base_url: str = "http://localhost:4000",
        model: str = "local-worker",
    ):
        self.client = AsyncOpenAI(base_url=base_url, api_key="not-needed")
        # Knowledge Context: Golden Snippet + ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ë²„ì „
        self._knowledge_context = generate_knowledge_context()
        self.model = model
        self.base_url = base_url
        
        # ê¸°ë³¸ ë„êµ¬ ì´ˆê¸°í™”
        self.tools_map = {tool.name: tool for tool in AVAILABLE_TOOLS}
        self.tool_schemas = [tool.to_schema() for tool in AVAILABLE_TOOLS]
        
        self.memory = global_memory  # Vector Memory ì—°ê²°

    async def initialize_mcp_tools(self, mcp_config: dict):
        """
        MCP ì„œë²„ì— ì—°ê²°í•˜ê³  ë„êµ¬ë¥¼ ê°€ì ¸ì™€ ì‹œìŠ¤í…œì— ë“±ë¡í•©ë‹ˆë‹¤.
        """
        if not mcp_config:
            return

        logger.info("ğŸ”Œ initializing MCP Tools...")
        
        # 1. Connect to Servers
        for name, config in mcp_config.items():
            if not config: continue
            
            command = config.get("command")
            args = config.get("args", [])
            env = config.get("env", {})
            
            if command:
                await global_mcp_manager.connect_server(name, command, args, env)

        # 2. Fetch Tools
        mcp_tools = await global_mcp_manager.get_tools()
        
        # 3. Register Tools
        count = 0
        for tool in mcp_tools:
            self.tools_map[tool.name] = tool
            self.tool_schemas.append(tool.to_schema())
            count += 1
            
        logger.info(f"âœ… MCP Tools initialized: {count} tools added.")


    async def execute(
        self,
        task: str,
        context: list[dict] | None = None,
    ) -> dict:
        """
        ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (RAG + MCP ì ìš©)
        """
        # â”€â”€ 0ë‹¨ê³„: Recall (ê¸°ì–µ ì¸ì¶œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        relevant_memories = self.memory.search(task, top_k=3)
        retrieved_context = ""
        if relevant_memories:
            logger.info(f"ğŸ§  [Recall] ê´€ë ¨ ê¸°ì–µ {len(relevant_memories)}ê°œ ì¸ì¶œ ì„±ê³µ")
            retrieved_context = "\n".join([
                f"- [Past Experience] {m['text']} (ìœ ì‚¬ë„: {1 - m['distance']:.2f})"
                for m in relevant_memories
            ])
        
        # â”€â”€ 1ë‹¨ê³„: ë‹¨ìˆœ ì‘ì—…ì¸ì§€ íŒë³„í•˜ì—¬ Helper ìœ„ì„ ì‹œë„ â”€â”€â”€â”€
        helper_result = None
        helper_used = False
        helper_fallback = False

        if self._is_helper_delegatable(task):
            logger.info("ğŸ“‹ ë‹¨ìˆœ ë°˜ë³µ ì‘ì—… ê°ì§€ â†’ Helperì— ìœ„ì„ ì‹œë„")
            helper_result = await self._call_helper(task)

            if helper_result is not None:
                helper_used = True
                logger.info("âœ… Helperê°€ ì‘ì—…ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í•¨")
            else:
                helper_fallback = True
                logger.info(
                    "âš ï¸ Helper ì‹¤íŒ¨ â†’ Workerê°€ ì§ì ‘ ì²˜ë¦¬ (Do It Yourself)"
                )

        # â”€â”€ 2ë‹¨ê³„: Worker ë³¸ì¸ì´ ì²˜ë¦¬ (Tool Use + RAG Context) 
        worker_response = await self._generate_response(
            task=task,
            context=context,
            helper_result=helper_result,
            helper_fallback=helper_fallback,
            retrieved_context=retrieved_context,
        )

        # [ESCALATE] ì²´í¬ (ê²€ì¦ ì „ì— ë¨¼ì € í™•ì¸)
        if worker_response is None or "[ESCALATE]" in (worker_response or ""):
            if "[ESCALATE]" in (worker_response or ""):
                logger.warning("ğŸš¨ Workerê°€ ì—ìŠ¤ì»¬ë ˆì´ì…˜ì„ ìš”ì²­í•¨: [ESCALATE]")
            return {
                "response": worker_response or "[ERROR] Worker failed",
                "escalated": True,
                "helper_used": helper_used,
                "helper_fallback": helper_fallback,
                "validation_passed": False,
                "critic_passed": None,
            }

        # â”€â”€ 3ë‹¨ê³„: Layer 1 â€” ê²°ì •ë¡ ì  ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        validation = validate_response(worker_response)
        validation_passed = validation.valid

        if not validation_passed:
            logger.warning("âŒ [Layer 1] ê²°ì •ë¡ ì  ê²€ì¦ ì‹¤íŒ¨ â†’ ì¬ì‹œë„ ì‹œì‘")
            worker_response, validation_passed = await self._retry_with_feedback(
                task=task,
                context=context,
                validation=validation,
                helper_result=helper_result,
                helper_fallback=helper_fallback,
            )

            # ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ â†’ ê°•ì œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
            if not validation_passed:
                logger.error("ğŸš¨ [Layer 1] ì¬ì‹œë„ ì‹¤íŒ¨ â†’ Cloud PM ê°•ì œ ì—ìŠ¤ì»¬ë ˆì´ì…˜")
                return {
                    "response": worker_response,
                    "escalated": True,
                    "helper_used": helper_used,
                    "helper_fallback": helper_fallback,
                    "validation_passed": False,
                    "critic_passed": None,
                }

        # â”€â”€ 4ë‹¨ê³„: Layer 2 â€” Critic Agent (ë©€í‹°í„´) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        critic_passed = None
        if validation.has_code:
            logger.info("ğŸ” [Layer 2] Critic Agent ê²€ì¦ ì‹œì‘ (ë©€í‹°í„´)...")
            worker_response, critic_passed = await self._critic_loop(
                response=worker_response,
                task=task,
                context=context,
                helper_result=helper_result,
                helper_fallback=helper_fallback,
            )

            if not critic_passed:
                logger.warning(
                    "âŒ [Layer 2] Critic ë©€í‹°í„´ ë£¨í”„ ì‹¤íŒ¨ â†’ Cloud PM ì—ìŠ¤ì»¬ë ˆì´ì…˜"
                )
                return {
                    "response": worker_response,
                    "escalated": True,
                    "helper_used": helper_used,
                    "helper_fallback": helper_fallback,
                    "validation_passed": validation_passed,
                    "critic_passed": False,
                }
            else:
                logger.info("âœ… [Layer 2] Critic ê²€ì¦ í†µê³¼")
                
                # [NEW] ì„±ê³µì ì¸ ê²°ê³¼ ê¸°ì–µ (Memorize)
                self._memorize_success(task, worker_response)

        # â”€â”€ ì „ì²´ í†µê³¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        return {
            "response": worker_response,
            "escalated": False,
            "helper_used": helper_used,
            "helper_fallback": helper_fallback,
            "validation_passed": validation_passed,
            "critic_passed": critic_passed,
        }

    def _memorize_success(self, task: str, response: str):
        """ì„±ê³µí•œ ì‘ì—… ë‚´ìš©ì„ ë²¡í„° ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            snippet = f"Task: {task}\n\nSolution:\n{response[:1000]}"
            self.memory.add(snippet, metadata={"type": "solution", "task": task})
            logger.info("ğŸ’¾ [Memorize] ì„±ê³µì ì¸ í•´ê²°ì±…ì„ ì¥ê¸° ê¸°ì–µì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°ì–µ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _critic_loop(
        self,
        response: str,
        task: str,
        context: list[dict] | None,
        helper_result: str | None,
        helper_fallback: bool,
        max_rounds: int = MAX_CRITIC_ROUNDS,
    ) -> tuple[str, bool]:
        """Criticê³¼ì˜ ëŒ€í™”í˜• ë¹„í‰ ë£¨í”„."""
        current_response = response

        for round_num in range(1, max_rounds + 1):
            critic_result = await critique(
                response=current_response,
                task=task,
                base_url=self.base_url,
            )

            if critic_result["passed"]:
                return current_response, True

            suggestions = critic_result.get("suggestions", [critic_result["reason"]])
            reason = critic_result["reason"]
            logger.warning(
                f"âŒ [Critic Round {round_num}/{max_rounds}] REJECT: {reason[:200]}"
            )

            if round_num < max_rounds:
                feedback = (
                    f"Critic í”¼ë“œë°± (Round {round_num}):\n"
                    f"íŒì •: REJECT\n"
                    f"ì‚¬ìœ : {reason}\n"
                    f"ìˆ˜ì • ì œì•ˆ:\n"
                    + "\n".join(f"- {s}" for s in suggestions)
                    + "\n\nìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì‘ë‹µì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
                )

                new_response = await self._generate_response(
                    task=task,
                    context=context,
                    helper_result=helper_result,
                    helper_fallback=helper_fallback,
                    error_feedback=feedback,
                )

                if new_response is None or "[ESCALATE]" in (new_response or ""):
                    return current_response, False

                current_response = new_response
                logger.info(
                    f"ğŸ”„ [Critic Round {round_num}] Worker ì‘ë‹µ ì¬ìƒì„± ì™„ë£Œ â†’ ì¬í‰ê°€"
                )

        return current_response, False

    async def _generate_response(
        self,
        task: str,
        context: list[dict] | None,
        helper_result: str | None,
        helper_fallback: bool,
        error_feedback: str | None = None,
        retrieved_context: str = "",
    ) -> str | None:
        """
        Worker LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. (Tool Use Loop í¬í•¨)
        """
        messages = self._build_messages(
            task=task,
            context=context,
            helper_result=helper_result,
            helper_fallback=helper_fallback,
            retrieved_context=retrieved_context,
        )

        if error_feedback:
            messages.append({
                "role": "user",
                "content": error_feedback,
            })

        # â”€â”€ Tool Use Loop (Re-act) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tracker = TokenUsageTracker(agent_name=self.model)
        for step in range(MAX_TOOL_STEPS):
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.4,
                    max_tokens=4096,
                    tools=self.tool_schemas,  # ë™ì  ë„êµ¬ í¬í•¨
                    tool_choice="auto",
                )
                
                # ìˆ˜ë™ í† í° íŠ¸ë˜í‚¹ íŒŒì‹±
                if hasattr(response, "usage") and response.usage:
                    prompt_info = getattr(response.usage, "prompt_tokens", 0)
                    completion_info = getattr(response.usage, "completion_tokens", 0)
                    mock_res = type('Result', (), {'llm_output': {'token_usage': {'prompt_tokens': prompt_info, 'completion_tokens': completion_info}, 'model_name': self.model}})()
                    tracker.on_llm_end(mock_res)
                
                msg = response.choices[0].message
                content = msg.content or ""
                tool_calls = msg.tool_calls

                if not tool_calls:
                    return content

                messages.append(msg)
                
                for tool_call in tool_calls:
                    func_name = tool_call.function.name
                    func_args = json.loads(tool_call.function.arguments)
                    
                    logger.info(f"ğŸ› ï¸ Tool Call: {func_name}({func_args})")
                    
                    # ë„êµ¬ ì¡°íšŒ (Static + MCP)
                    tool = self.tools_map.get(func_name)
                    
                    if tool:
                        result = await tool.validate_and_execute(**func_args)
                    else:
                        result = f"âŒ Error: Tool '{func_name}' not found."
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": str(result),
                    })
                    logger.info(f"  â””â”€ Result: {str(result)[:100]}...")

            except Exception as e:
                logger.error(f"âŒ Worker ì‹¤í–‰ ì‹¤íŒ¨ (Step {step}): {e}")
                return None
        
        logger.warning(f"âš ï¸ Tool Loop Limit Reached ({MAX_TOOL_STEPS})")
        return None

    async def _retry_with_feedback(
        self,
        task: str,
        context: list[dict] | None,
        validation,
        helper_result: str | None,
        helper_fallback: bool,
    ) -> tuple[str, bool]:
        """ê²°ì •ë¡ ì  ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„."""
        last_response = ""
        first_errors = []

        for attempt in range(1, MAX_VALIDATION_RETRIES + 1):
            logger.info(f"ğŸ”„ [Layer 1] ì¬ì‹œë„ {attempt}/{MAX_VALIDATION_RETRIES}")

            if attempt == 1:
                first_errors = list(validation.errors)

            error_feedback = format_error_feedback(validation)
            new_response = await self._generate_response(
                task=task,
                context=context,
                helper_result=helper_result,
                helper_fallback=helper_fallback,
                error_feedback=error_feedback,
            )

            if new_response is None:
                last_response = "[ERROR] Worker retry failed"
                break

            last_response = new_response
            new_validation = validate_response(new_response)

            if new_validation.valid:
                logger.info(f"âœ… [Layer 1] ì¬ì‹œë„ {attempt}íšŒì—ì„œ ê²€ì¦ í†µê³¼!")
                
                if first_errors and new_validation.code_blocks:
                    # Record Learning
                    record_learning(
                        error_message="; ".join(first_errors),
                        original_code="(ê²€ì¦ ì‹¤íŒ¨í•œ ì½”ë“œ)",
                        fixed_code=new_validation.code_blocks[0][:300],
                    )
                    self._memorize_success(task, new_response)

                return new_response, True

            validation = new_validation

        return last_response, False

    def _is_helper_delegatable(self, task: str) -> bool:
        task_lower = task.lower()
        return any(keyword in task_lower for keyword in HELPER_DELEGATABLE_KEYWORDS)

    async def _call_helper(self, task: str) -> str | None:
        return await ask_helper_safe(task, max_retries=3, base_url=self.base_url)

    def _build_messages(
        self,
        task: str,
        context: list[dict] | None,
        helper_result: str | None,
        helper_fallback: bool,
        retrieved_context: str = "",
    ) -> list[dict]:
        """Workerì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ ëª©ë¡ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        system_prompt = WORKER_SYSTEM_PROMPT
        
        # [NEW] Long-term Memory Injection
        if retrieved_context:
            system_prompt += f"\n\n## [Long-term Memory] ìœ ì‚¬í•œ ê³¼ê±° ê²½í—˜\n{retrieved_context}"
            
        if self._knowledge_context:
            system_prompt += "\n\n" + self._knowledge_context

        messages = [{"role": "system", "content": system_prompt}]

        if context:
            messages.extend(context)

        if helper_result is not None:
            user_content = (
                f"ì‘ì—… ìš”ì²­: {task}\n\n"
                f"ë³´ì¡° ë„êµ¬(Helper)ê°€ ë‹¤ìŒ ê²°ê³¼ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤:\n"
                f"--- Helper ê²°ê³¼ ---\n{helper_result}\n--- ë ---"
            )
        elif helper_fallback:
            user_content = (
                f"ì‘ì—… ìš”ì²­: {task}\n\n"
                f"[ì°¸ê³ ] Helperê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ì²˜ë¦¬í•˜ì‹­ì‹œì˜¤."
            )
        else:
            user_content = task

        messages.append({"role": "user", "content": user_content})
        return messages
