# ============================================================
# M4 128GB 전용 하드웨어 프로파일 (Enterprise / Research)
# ============================================================
# Apple Silicon 128GB (M4 Ultra/Max) — "앙상블 지능과 인메모리 RAG"
# 최적화 대상: Qwen2.5-72B-Instruct-8bit (고정밀) + In-Memory RAG
# ============================================================

hardware:
  chip: "m4_ultra"
  memory_gb: 128
  cpu_cores: 32
  p_cores: 16
  e_cores: 16
  gpu_cores: 80
  memory_bandwidth_gbps: 800

# ── MLX 추론 설정 ──────────────────────────────────────────────
mlx:
  main_model: "mlx-community/Qwen2.5-72B-Instruct-8bit"
  draft_model: "mlx-community/Qwen2.5-1.5B-Instruct-4bit"
  speculative_decoding: true
  draft_steps: 5
  kv_cache_bits: 8
  max_context_length: 131072
  max_tokens: 8192
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

# ── 메모리 맵 (128GB 기준) ────────────────────────────────────
# 구성 요소           | 메모리 (GB) | 비고
# --------------------|-------------|---------------------------
# 메인 모델 (72B Q8)  | 75.0        | FP16에 근접한 정밀도
# 드래프트 모델 (1.5B)| 1.0         | 투기적 디코딩용
# KV Cache (8-bit)    | 18.0        | 컨텍스트 128k 기준
# In-Memory RAG Cache | 10.0        | 벡터 인덱스 캐싱
# OpenClaw Runtime    | 3.0         | Node.js + Gateway (확장)
# macOS               | 8.0         | Kernel + WindowServer + Heavy Apps
# 여유 공간           | 13.0        | 대규모 작업 스파이크 방지
# --------------------|-------------|---------------------------
# 합계                | 128.0       |

memory_limits:
  model_budget_gb: 80.0             # 모델 + 드래프트 한도
  kv_cache_budget_gb: 20.0          # KV Cache 한도 (대용량)
  fallback_threshold_gb: 10.0       # 이 이하이면 경량 모델 전환
  wired_limit_mb: 110592            # sysctl iogpu.wired_limit_mb 권장값 (108GB)

# ── Context Pruning ───────────────────────────────────────────
context_pruning:
  mode: "token-budget"
  budget: 128000                    # 128k 토큰 (책 한 권 분량)
  keep_system_prompt: true

# ── Vision ─────────────────────────────────────────────────────
vision:
  image_max_dimension_px: 2048      # 원본 화질 유지

# ── In-Memory RAG 설정 ────────────────────────────────────────
rag:
  enabled: true
  backend: "sqlite-vss"             # 로컬 벡터 검색
  cache_size_gb: 10                 # 벡터 인덱스를 RAM에 캐싱
  embedding_model: "all-MiniLM-L6-v2"

# ── Fallback 모델 (메모리 부족 시) ─────────────────────────────
fallback:
  model: "mlx-community/Llama-3.3-70B-Instruct-4bit"
  draft_model: "mlx-community/Llama-3.2-1B-Instruct-4bit"
  speculative_decoding: true
  kv_cache_bits: 4
  max_context_length: 32768

# ── QoS 설정 ──────────────────────────────────────────────────
qos:
  prefer_performance_cores: true
  taskpolicy_level: "default"

# ── Sandbox 설정 ──────────────────────────────────────────────
sandbox:
  mode: "docker"
  memory_limit: "16gb"              # 도구 실행에 넉넉한 메모리 할당
  timeout: "600s"                   # 장시간 복잡 연산 허용
