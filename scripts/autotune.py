#!/usr/bin/env python3
"""
AutoTune â€” ìê°€ ì ì‘í˜• í•˜ë“œì›¨ì–´-ëª¨ë¸ ìµœì í™” ìŠ¤í‚¬
=====================================================
ì‹œìŠ¤í…œ RAMì„ ì§„ë‹¨í•˜ê³ , HuggingFace Hubì—ì„œ ìµœì‹  MLX í˜¸í™˜ ëª¨ë¸ì„
íƒìƒ‰í•˜ì—¬ ìµœì ì˜ êµ¬ì„±ì„ ì¶”ì²œ/ì ìš©í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/autotune.py --mode check       # í˜„ì¬ ìƒíƒœ + ì¶”ì²œ (ë³€ê²½ ì—†ìŒ)
    python scripts/autotune.py --mode update      # ìµœì  ëª¨ë¸ë¡œ ì„¤ì • ë³€ê²½
    python scripts/autotune.py --type coder       # ì½”ë”© íŠ¹í™” ëª¨ë¸ ìš°ì„  íƒìƒ‰
    python scripts/autotune.py --tier 32gb        # íŠ¹ì • í‹°ì–´ ê°•ì œ
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from dataclasses import dataclass
from typing import Optional

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("autotune")

# â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG_DIR = os.path.join(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config"
)
SYSTEM_RESERVE_GB = 4.0  # OS + ëŸ°íƒ€ì„ ì˜ˆì•½ ë©”ëª¨ë¦¬

# RAM í‹°ì–´ë³„ ëª¨ë¸ í¬ê¸° ìƒí•œ (GB ë‹¨ìœ„)
TIER_BUDGETS: dict[str, float] = {
    "16gb": 10.0,   # ëª¨ë¸ ì˜ˆì‚° 10GB (16 - 4 OS - 2 ì—¬ìœ )
    "32gb": 22.0,   # ëª¨ë¸ ì˜ˆì‚° 22GB
    "64gb": 44.0,   # ëª¨ë¸ ì˜ˆì‚° 44GB
    "128gb": 80.0,  # ëª¨ë¸ ì˜ˆì‚° 80GB
}


# â”€â”€ ë°ì´í„° í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class ModelCandidate:
    """HuggingFaceì—ì„œ íƒìƒ‰ëœ ëª¨ë¸ í›„ë³´."""

    model_id: str
    size_est_gb: float
    downloads: int
    likes: int
    quantization: str = "4bit"


# â”€â”€ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_system_ram_gb() -> float:
    """ì´ ë¬¼ë¦¬ì  RAM ìš©ëŸ‰ì„ GB ë‹¨ìœ„ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        import psutil  # type: ignore[import-untyped]
        mem = psutil.virtual_memory()
        return round(mem.total / (1024 ** 3), 1)
    except ImportError:
        # macOS fallback
        import subprocess
        try:
            result = subprocess.run(
                ["sysctl", "-n", "hw.memsize"],
                capture_output=True, text=True, timeout=5,
            )
            return round(int(result.stdout.strip()) / (1024 ** 3), 1)
        except Exception:
            return 0.0


def detect_ram_tier(ram_gb: float) -> str:
    """RAM ìš©ëŸ‰ì— ë”°ë¥¸ í‹°ì–´ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if ram_gb >= 128:
        return "128gb"
    elif ram_gb >= 64:
        return "64gb"
    elif ram_gb >= 32:
        return "32gb"
    else:
        return "16gb"


# â”€â”€ ëª¨ë¸ í¬ê¸° ì¶”ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ëª¨ë¸ ì´ë¦„ì—ì„œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ íŒŒì‹±í•˜ê³  ì–‘ìí™” ë¹„íŠ¸ì— ë§ê²Œ ë©”ëª¨ë¦¬ ì¶”ì •
PARAM_SIZE_MAP: list[tuple[str, float]] = [
    # (íŒ¨í„´, 4-bit ì–‘ìí™” ê¸°ì¤€ GB)
    ("405B", 230.0),
    ("72B", 42.0),
    ("70B", 40.0),
    ("32B", 19.5),
    ("24B", 14.0),
    ("14B", 9.0),
    ("13B", 8.5),
    ("8B", 6.0),
    ("7B", 5.0),
    ("3B", 2.2),
    ("1.5B", 1.2),
    ("1B", 0.7),
    ("0.5B", 0.4),
]


def estimate_model_size_gb(model_name: str) -> float:
    """ëª¨ë¸ ì´ë¦„ì—ì„œ 4-bit ê¸°ì¤€ ë©”ëª¨ë¦¬ ì ìœ ëŸ‰(GB)ì„ ì¶”ì •í•©ë‹ˆë‹¤."""
    name_upper = model_name.upper()
    for pattern, size_gb in PARAM_SIZE_MAP:
        if pattern in name_upper:
            # 8-bit ëª¨ë¸ì€ 2ë°°
            if "8bit" in model_name.lower() or "8-bit" in model_name.lower():
                return size_gb * 2.0
            # 3-bit ëª¨ë¸ì€ 0.75ë°°
            if "3bit" in model_name.lower() or "3-bit" in model_name.lower():
                return size_gb * 0.75
            return size_gb
    return 0.0


# â”€â”€ HuggingFace ëª¨ë¸ íƒìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def find_best_models(
    max_model_gb: float,
    task_type: str = "instruct",
    limit: int = 50,
) -> list[ModelCandidate]:
    """HuggingFaceì—ì„œ RAM ìš©ëŸ‰ ë‚´ êµ¬ë™ ê°€ëŠ¥í•œ ìµœì‹  MLX ëª¨ë¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        max_model_gb: ëª¨ë¸ì— í• ë‹¹ ê°€ëŠ¥í•œ ìµœëŒ€ ë©”ëª¨ë¦¬ (GB)
        task_type: "instruct" ë˜ëŠ” "coder"
        limit: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜

    Returns:
        í¬ê¸° â†’ ë‹¤ìš´ë¡œë“œ ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ìµœëŒ€ 5ê°œ í›„ë³´ ëª©ë¡
    """
    try:
        from huggingface_hub import HfApi  # type: ignore[import-untyped]
    except ImportError:
        logger.error(
            "âŒ huggingface_hub íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: "
            "pip install huggingface_hub"
        )
        return []

    api = HfApi()
    candidates: list[ModelCandidate] = []

    try:
        models = api.list_models(
            search="mlx-community",
            sort="downloads",
            direction=-1,
            limit=limit,
        )
    except Exception as e:
        logger.error(f"âŒ HuggingFace API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []

    for m in models:
        name = m.modelId or ""

        # ì–‘ìí™” í•„í„°: 4bit ë˜ëŠ” 8bitë§Œ
        is_4bit = "4bit" in name.lower()
        is_8bit = "8bit" in name.lower()
        if not (is_4bit or is_8bit):
            continue

        # Instruct ëª¨ë¸ í•„í„°
        if "instruct" not in name.lower() and task_type == "instruct":
            continue

        # Coder í•„í„°
        if task_type == "coder" and "coder" not in name.lower():
            continue

        size_gb = estimate_model_size_gb(name)
        if size_gb <= 0:
            continue

        # ê°€ìš© ë©”ëª¨ë¦¬ ë²”ìœ„ ë‚´ í•„í„°
        if size_gb > max_model_gb:
            continue

        quant = "8bit" if is_8bit else "4bit"
        candidates.append(ModelCandidate(
            model_id=name,
            size_est_gb=size_gb,
            downloads=m.downloads or 0,
            likes=m.likes or 0,
            quantization=quant,
        ))

    # í¬ê¸°(ì§€ëŠ¥) ìš°ì„ , ë™ì¼ í¬ê¸°ë©´ ë‹¤ìš´ë¡œë“œ ìˆ˜ ìˆœ
    candidates.sort(
        key=lambda x: (x.size_est_gb, x.downloads), reverse=True
    )
    return candidates[:5]


# â”€â”€ êµ¬ì„± ì—…ë°ì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_current_config(tier: str) -> Optional[dict[str, object]]:
    """í˜„ì¬ í‹°ì–´ì˜ YAML ì„¤ì •ì„ ì½ìŠµë‹ˆë‹¤."""
    import yaml

    profile_name = f"m4_{tier}"
    config_path = os.path.join(CONFIG_DIR, f"{profile_name}.yaml")
    if not os.path.exists(config_path):
        return None
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except Exception as e:
        logger.error(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def update_config(tier: str, model_id: str) -> bool:
    """í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼ YAMLì—ì„œ main_modelì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    import yaml

    profile_name = f"m4_{tier}"
    config_path = os.path.join(CONFIG_DIR, f"{profile_name}.yaml")

    if not os.path.exists(config_path):
        logger.error(f"âŒ í”„ë¡œíŒŒì¼ íŒŒì¼ ì—†ìŒ: {config_path}")
        return False

    try:
        with open(config_path, "r", encoding="utf-8") as f:
            content = f.read()

        data = yaml.safe_load(content) or {}
        current_model = data.get("mlx", {}).get("main_model", "")

        if current_model == model_id:
            logger.info(f"âœ… ì´ë¯¸ ìµœì  ëª¨ë¸({model_id})ì„ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.")
            return True

        # main_model ê°’ë§Œ êµì²´ (YAML í¬ë§· ìœ ì§€)
        new_content = content.replace(
            f'main_model: "{current_model}"',
            f'main_model: "{model_id}"',
        )

        with open(config_path, "w", encoding="utf-8") as f:
            f.write(new_content)

        logger.info(
            f"âœ… ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ: {current_model} â†’ {model_id}"
        )
        return True

    except Exception as e:
        logger.error(f"âŒ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False


# â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def main() -> None:
    parser = argparse.ArgumentParser(
        description="AgenticFlow ìê°€ ì ì‘í˜• ëª¨ë¸ ìµœì í™” ìŠ¤í‚¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            "ì˜ˆì‹œ:\n"
            "  python scripts/autotune.py --mode check\n"
            "  python scripts/autotune.py --mode update --type coder\n"
            "  python scripts/autotune.py --tier 64gb --mode check\n"
        ),
    )
    parser.add_argument(
        "--mode",
        choices=["check", "update"],
        default="check",
        help="check: í˜„ì¬ ìƒíƒœ + ì¶”ì²œ / update: ì„¤ì • ìë™ ë³€ê²½",
    )
    parser.add_argument(
        "--type",
        choices=["instruct", "coder"],
        default="instruct",
        help="íƒìƒ‰í•  ëª¨ë¸ ìœ í˜• (ê¸°ë³¸: instruct)",
    )
    parser.add_argument(
        "--tier",
        choices=["16gb", "32gb", "64gb", "128gb"],
        default=None,
        help="íŠ¹ì • RAM í‹°ì–´ ê°•ì œ (ë¯¸ì§€ì • ì‹œ ìë™ ê°ì§€)",
    )
    args = parser.parse_args()

    # â”€â”€ 1. ì‹œìŠ¤í…œ ìì› ì„±ì°° (Introspect) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_ram = get_system_ram_gb()
    tier = args.tier or detect_ram_tier(total_ram)
    model_budget = TIER_BUDGETS.get(tier, 10.0)

    print(f"\n{'=' * 60}")
    print(f"  ğŸ” System Auto-Tune Report")
    print(f"{'=' * 60}")
    print(f"  ì‹œìŠ¤í…œ RAM   : {total_ram:.1f} GB")
    print(f"  ê°ì§€ëœ í‹°ì–´  : {tier}")
    print(f"  ëª¨ë¸ ì˜ˆì‚°    : {model_budget:.1f} GB")

    # í˜„ì¬ ì„¤ì • í‘œì‹œ
    current = get_current_config(tier)
    if current:
        mlx_cfg = current.get("mlx", {})
        print(f"  í˜„ì¬ ëª¨ë¸    : {mlx_cfg.get('main_model', 'N/A')}")
        print(f"  ë“œë˜í”„íŠ¸     : {mlx_cfg.get('draft_model', 'N/A')}")
        print(
            f"  íˆ¬ê¸°ì  ë””ì½”ë”©: "
            f"{'âœ…' if mlx_cfg.get('speculative_decoding') else 'âŒ'}"
        )
        print(f"  ì»¨í…ìŠ¤íŠ¸     : {mlx_cfg.get('max_context_length', 'N/A')}")

    # â”€â”€ 2. ëª¨ë¸ íƒìƒ‰ (Scout) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n  ğŸ“¡ HuggingFaceì—ì„œ MLX ëª¨ë¸ íƒìƒ‰ ì¤‘...")
    print(f"     (í•„í„°: {args.type}, ìµœëŒ€ {model_budget:.0f}GB)")

    candidates = find_best_models(model_budget, args.type)

    if not candidates:
        print("  âš ï¸ ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"{'=' * 60}\n")
        return

    # â”€â”€ 3. í‰ê°€ ê²°ê³¼ í‘œì‹œ (Evaluate) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n  ğŸ† ì¶”ì²œ ëª¨ë¸ (ìƒìœ„ 5ê°œ):")
    print(f"  {'â”€' * 56}")
    for i, c in enumerate(candidates):
        marker = " ğŸ‘ˆ ìµœì " if i == 0 else ""
        print(
            f"  {i + 1}. {c.model_id}\n"
            f"     ë©”ëª¨ë¦¬: ~{c.size_est_gb:.1f}GB ({c.quantization}) | "
            f"ë‹¤ìš´ë¡œë“œ: {c.downloads:,} | "
            f"ì¢‹ì•„ìš”: {c.likes:,}{marker}"
        )

    # â”€â”€ 4. ì ì‘ (Adapt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.mode == "update" and candidates:
        best = candidates[0]
        print(f"\n  ğŸ”§ ìµœì  ëª¨ë¸ ìë™ ì ìš© ì¤‘: {best.model_id}")
        success = update_config(tier, best.model_id)
        if success:
            print("  âœ… êµ¬ì„± ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print("  â„¹ï¸  ë³€ê²½ ì‚¬í•­ ì ìš©ì„ ìœ„í•´ ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”.")
        else:
            print("  âŒ êµ¬ì„± ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    elif args.mode == "check":
        print(f"\n  â„¹ï¸  ì ìš©í•˜ë ¤ë©´ --mode update ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

    print(f"{'=' * 60}\n")


if __name__ == "__main__":
    main()
