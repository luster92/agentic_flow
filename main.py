"""
Mac Mini M4 Hybrid AI Orchestration System
===========================================
ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°: ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ 4ê³„ì¸µ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì¡°ìœ¨í•©ë‹ˆë‹¤.
ë©€í‹° í”„ë¡œì íŠ¸(Contextualized Framework) ì§€ì›:
- /new <project>: ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
- /load <project>: í”„ë¡œì íŠ¸ ì „í™˜
- /list: í”„ë¡œì íŠ¸ ëª©ë¡
- /current: í˜„ì¬ ìƒíƒœ

Cloud PM ì „í™˜:
- /model <name>: í´ë¼ìš°ë“œ ëª¨ë¸ ë³€ê²½ (gemini, claude, gpt4)

ì›Œí”Œë¡œìš°:
1. ì‚¬ìš©ì ì…ë ¥ â†’ Router(DeepSeek-R1)ê°€ ë¶„ì„
2. LOCAL ë¼ìš°íŒ… â†’ Worker(Qwen 32B)ê°€ ì‹¤í–‰
3. CLOUD ë¼ìš°íŒ… ë˜ëŠ” [ESCALATE] â†’ Cloud PM(Gemini/Claude 4.6/GPT-5.3)ì´ ì²˜ë¦¬
4. ê²°ê³¼ ë°˜í™˜ ë° ëŒ€í™” ê¸°ë¡ ì €ì¥

ì‹¤í–‰: python main.py
"""

import os
import sys
import asyncio
import logging
import yaml
from dotenv import load_dotenv
from openai import AsyncOpenAI

from agents.router import Router
from agents.worker import Worker
from state import AgenticState
from utils.history_manager import HistoryManager, DEFAULT_HISTORY_DIR
from utils.mcp_client import global_mcp_manager
from utils.semantic_cache import SemanticCache

# â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

LITELLM_BASE_URL = os.getenv("LITELLM_BASE_URL", "http://localhost:4000")
CONTEXT_WINDOW = int(os.getenv("CONTEXT_WINDOW_SIZE", "20"))
HISTORY_DIR = os.getenv("HISTORY_DIR", "history")
CONFIG_FILE = "config.yaml"

# ê¸°ë³¸ í´ë¼ìš°ë“œ ëª¨ë¸
CLOUD_MODEL_NAME = "cloud-pm-gemini"

# ëª¨ë¸ ë§¤í•‘ (ë‹¨ì¶•ì–´ -> ì‹¤ì œ ëª¨ë¸ëª…)
MODEL_MAP = {
    "gemini": "cloud-pm-gemini",
    "claude": "cloud-pm-claude",
    "gpt4": "cloud-pm-gpt4",
}

# â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(name)-20s â”‚ %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("orchestrator")


def load_config() -> dict:
    """config.yaml íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(CONFIG_FILE):
        return {}
    try:
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except Exception as e:
        logger.error(f"âŒ Failed to load config.yaml: {e}")
        return {}


async def call_cloud_pm(
    task: str,
    context: list[dict] | None = None,
    base_url: str = LITELLM_BASE_URL,
    stream: bool = True,
) -> str:
    """Cloud PM í˜¸ì¶œ (ë™ì  ëª¨ë¸ ì„ íƒ, ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
    client = AsyncOpenAI(base_url=base_url, api_key="not-needed")
    model_name = CLOUD_MODEL_NAME

    messages = [
        {
            "role": "system",
            "content": (
                "You are a senior project manager and architect with deep expertise "
                "in software design, complex reasoning, and strategic planning. "
                "Provide thorough, well-structured solutions."
            ),
        },
    ]

    if context:
        messages.extend(context)

    messages.append({"role": "user", "content": task})

    try:
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: ì²« í† í°ë¶€í„° ì¦‰ì‹œ ì¶œë ¥
            response_stream = await client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.5,
                max_tokens=4096,
                stream=True,
            )
            chunks = []
            print("\nğŸ¤– Assistant > ", end="", flush=True)
            async for chunk in response_stream:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                chunks.append(content)
            print()  # ì¤„ë°”ê¿ˆ
            return "".join(chunks) or "[Cloud PM returned empty response]"
        else:
            response = await client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.5,
                max_tokens=4096,
            )
            return response.choices[0].message.content or "[Cloud PM returned empty response]"
    except Exception as e:
        logger.error(f"âŒ Cloud PM í˜¸ì¶œ ì‹¤íŒ¨ ({model_name}): {e}")
        return f"[ERROR] Cloud PM ({model_name}) failed: {e}"


async def process_request(
    user_input: str,
    router: Router,
    worker: Worker,
    history: HistoryManager,
    state: AgenticState,
    cache: SemanticCache | None = None,
) -> str:
    """ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (AgenticState ê¸°ë°˜)"""
    history.add_message("user", user_input)
    state.increment_turn()

    # â”€â”€ 0ë‹¨ê³„: Semantic Cache Lookup (Short-Circuit) â”€â”€â”€â”€â”€â”€
    if cache:
        cached = cache.get(user_input)
        if cached is not None:
            history.add_message(
                "assistant", cached,
                metadata={"handler": "semantic-cache", "cache_hit": True},
            )
            return cached

    # â”€â”€ 1ë‹¨ê³„: Sticky Routing ë˜ëŠ” Router í˜¸ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("=" * 60)

    if state.current_agent is not None:
        # Sticky Routing: ì´ì „ í„´ì˜ ì—ì´ì „íŠ¸ë¥¼ ìœ ì§€
        destination = state.current_agent
        reason = "Sticky Routing (ì´ì „ í„´ê³¼ ë™ì¼ ì—ì´ì „íŠ¸)"
        logger.info(f"ğŸ§­ [Sticky Route] Router ìŠ¤í‚µ â†’ {destination} | {reason}")
    else:
        # ìƒˆ ë¼ìš°íŒ… ê²°ì • í•„ìš”
        logger.info("ğŸ§­ [Router] ì‘ì—… ë¶„ì„ ì¤‘...")
        routing = await router.route(user_input)
        destination = routing["destination"]
        reason = routing["reason"]
        state.current_agent = destination
        logger.info(f"ğŸ§­ [Router] ê²°ì •: {destination} | ì‚¬ìœ : {reason}")

    history.add_message(
        "system",
        f"[ROUTING] {destination}: {reason}",
        metadata={"type": "routing", "sticky": state.current_agent is not None},
    )

    # â”€â”€ 2ë‹¨ê³„: ë¼ìš°íŒ…ì— ë”°ë¼ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context = history.get_context()
    state.conversation_history = context

    if destination == "CLOUD":
        logger.info(f"â˜ï¸  [Cloud PM: {CLOUD_MODEL_NAME}] ê³ ë‚œë„ ì‘ì—… ì²˜ë¦¬ ì¤‘...")
        final_response = await call_cloud_pm(user_input, context=context)
        history.add_message(
            "assistant", final_response,
            metadata={
                "handler": CLOUD_MODEL_NAME,
                "reason": reason,
                "streamed": True,
            },
        )
    else:
        logger.info("ğŸ”¨ [Worker] ì‘ì—… ì‹¤í–‰ ì¤‘...")
        result = await worker.execute(user_input, context=context)

        # Worker ì‹¤í–‰ ë©”íƒ€ ë¡œê¹…
        if result["helper_used"]:
            logger.info("  â””â”€ âœ… Helper í™œìš© ì™„ë£Œ")
        if result["helper_fallback"]:
            logger.info("  â””â”€ âš ï¸ Helper ì‹¤íŒ¨ â†’ Worker ì§ì ‘ ì²˜ë¦¬")

        # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
        v_passed = result.get("validation_passed", True)
        c_passed = result.get("critic_passed")
        if v_passed is False:
            logger.warning("  â””â”€ âŒ [Validator] ë¬¸ë²• ê²€ì¦ ì‹¤íŒ¨")
        if c_passed is False:
            logger.warning("  â””â”€ âŒ [Critic] ë¹„í‰ê°€ ê±°ì ˆ")

        if result["escalated"]:
            # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì‚¬ìœ  íŒë³„
            if c_passed is False:
                esc_reason = "critic-reject"
            elif v_passed is False:
                esc_reason = "validation-fail"
            else:
                esc_reason = "worker-escalation"

            logger.info(f"ğŸš¨ [Worker â†’ Cloud PM] ì—ìŠ¤ì»¬ë ˆì´ì…˜ ë°œìƒ! (ì‚¬ìœ : {esc_reason})")
            logger.info(f"â˜ï¸  [Cloud PM: {CLOUD_MODEL_NAME}] ë‚œì œ ì²˜ë¦¬ ì¤‘...")

            # Sticky Routing í•´ì œ: ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì‹œ ì—ì´ì „íŠ¸ ì „í™˜
            state.reset_routing()
            
            escalation_context = (
                f"ì´ì „ Workerì˜ ë¶„ì„:\n{result['response']}\n\n"
                f"ì›ë³¸ ìš”ì²­:\n{user_input}"
            )
            final_response = await call_cloud_pm(
                escalation_context, context=context
            )
            history.add_message(
                "assistant", final_response,
                metadata={
                    "handler": CLOUD_MODEL_NAME,
                    "reason": esc_reason,
                    "validation_passed": v_passed,
                    "critic_passed": c_passed,
                    "worker_response": result["response"][:500],
                },
            )
        else:
            final_response = result["response"]
            history.add_message(
                "assistant", final_response,
                metadata={
                    "handler": "local-worker",
                    "helper_used": result["helper_used"],
                    "helper_fallback": result["helper_fallback"],
                    "validation_passed": v_passed,
                    "critic_passed": c_passed,
                },
            )

    logger.info("=" * 60)

    # ì„±ê³µì ì¸ ì‘ë‹µì„ ìºì‹œì— ì €ì¥
    if cache and not final_response.startswith("[ERROR]"):
        cache.put(user_input, final_response)

    return final_response


def print_banner() -> None:
    """ì‹œì‘ ë°°ë„ˆ ì¶œë ¥"""
    banner = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ¤– Mac Mini M4 Hybrid AI Orchestration             â•‘
â•‘          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â•‘
â•‘   Commands:                                                  â•‘
â•‘    /new <project>    : ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± ë° ì „í™˜               â•‘
â•‘    /load <project>   : ê¸°ì¡´ í”„ë¡œì íŠ¸ ë¡œë“œ                     â•‘
â•‘    /model <name>     : Cloud PM ëª¨ë¸ ë³€ê²½ (gemini/claude...) â•‘
â•‘                        (Current: {CLOUD_MODEL_NAME})         â•‘
â•‘    /list             : í”„ë¡œì íŠ¸ ëª©ë¡ í™•ì¸                     â•‘
â•‘    /current          : í˜„ì¬ ìƒíƒœ í™•ì¸                         â•‘
â•‘    /clear            : ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”                       â•‘
â•‘    /stats            : ëŒ€í™” í†µê³„                              â•‘
â•‘    /exit             : ì¢…ë£Œ                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)


def switch_project(project_name: str) -> HistoryManager:
    """í”„ë¡œì íŠ¸ ì„¸ì…˜ ì „í™˜"""
    history = HistoryManager(
        project_name=project_name,
        base_dir=HISTORY_DIR,
        context_window=CONTEXT_WINDOW,
    )
    print(f"\nğŸ“‚ í”„ë¡œì íŠ¸ ì „í™˜ ì™„ë£Œ: [{project_name}]")
    logger.info(f"ğŸ“‚ Active Project switched to: {project_name}")
    return history


async def main() -> None:
    """ë©”ì¸ ë£¨í”„ (async)"""
    global CLOUD_MODEL_NAME
    print_banner()

    # â”€â”€ ì´ˆê¸° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not os.path.exists(HISTORY_DIR):
        os.makedirs(HISTORY_DIR)

    # Config ë¡œë“œ
    config = load_config()
    mcp_config = config.get("mcp_servers", {})

    router = Router(base_url=LITELLM_BASE_URL)
    worker = Worker(base_url=LITELLM_BASE_URL)
    
    # MCP ë„êµ¬ ì´ˆê¸°í™”
    if mcp_config:
        await worker.initialize_mcp_tools(mcp_config)

    # ê¸°ë³¸ í”„ë¡œì íŠ¸ë¡œ ì‹œì‘
    history = HistoryManager(
        project_name="default",
        base_dir=HISTORY_DIR,
        context_window=CONTEXT_WINDOW,
    )

    # AgenticState ì´ˆê¸°í™”
    state = AgenticState()

    # Semantic Cache ì´ˆê¸°í™”
    cache = SemanticCache()

    logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    logger.info(f"ğŸ“¡ LiteLLM Proxy: {LITELLM_BASE_URL}")

    # â”€â”€ ì¸í„°ë™í‹°ë¸Œ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        while True:
            try:
                prompt_text = f"\n[{history.project_name} | {CLOUD_MODEL_NAME.split('-')[-1]}] ğŸ§‘ You > "
                user_input = await asyncio.to_thread(input, prompt_text)
                user_input = user_input.strip()
            except EOFError:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not user_input:
                continue

            # â”€â”€ CLI ëª…ë ¹ì–´ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if user_input.startswith("/"):
                parts = user_input.split()
                cmd = parts[0].lower()
                args = parts[1:] if len(parts) > 1 else []

                if cmd in ("/exit", "/quit"):
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                elif cmd == "/clear":
                    history.clear()
                    state = AgenticState()  # ìƒíƒœë„ í•¨ê»˜ ì´ˆê¸°í™”
                    continue

                elif cmd == "/stats":
                    stats = history.get_stats()
                    print(f"\nğŸ“Š í†µê³„ ({stats['project']}):")
                    print(f"   ë©”ì‹œì§€ ìˆ˜: {stats['total_messages']}")
                    print(f"   íŒŒì¼ ê²½ë¡œ: {stats['file_path']}")
                    # í•¸ë“¤ëŸ¬ë³„ í†µê³„
                    full = history.get_full_history()
                    handlers: dict[str, int] = {}
                    for msg in full:
                        h = (msg.get("metadata") or {}).get("handler", "")
                        if h:
                            handlers[h] = handlers.get(h, 0) + 1
                    if handlers:
                        print("   í•¸ë“¤ëŸ¬ ë¶„í¬:")
                        for h, cnt in sorted(handlers.items(), key=lambda x: -x[1]):
                            print(f"     {h}: {cnt}íšŒ")
                    continue

                elif cmd == "/list":
                    projects = HistoryManager.list_projects(HISTORY_DIR)
                    print("\nğŸ“‚ ì €ì¥ëœ í”„ë¡œì íŠ¸ ëª©ë¡:")
                    for p in projects:
                        print(f"   - {p}")
                    continue

                elif cmd == "/current":
                    print(f"\nğŸ” í˜„ì¬ ìƒíƒœ:")
                    print(f"   í”„ë¡œì íŠ¸: {history.project_name}")
                    print(f"   Cloud PM: {CLOUD_MODEL_NAME}")
                    continue

                elif cmd == "/new":
                    if not args:
                        print("âš ï¸ ì‚¬ìš©ë²•: /new <project_name>")
                        continue
                    history = switch_project(args[0])
                    state = AgenticState()  # í”„ë¡œì íŠ¸ ì „í™˜ ì‹œ ìƒíƒœ ì´ˆê¸°í™”
                    continue

                elif cmd == "/load":
                    if not args:
                        print("âš ï¸ ì‚¬ìš©ë²•: /load <project_name>")
                        continue
                    history = switch_project(args[0])
                    state = AgenticState()  # í”„ë¡œì íŠ¸ ì „í™˜ ì‹œ ìƒíƒœ ì´ˆê¸°í™”
                    continue

                elif cmd == "/model":
                    if not args:
                        print(f"âš ï¸ ì‚¬ìš©ë²•: /model <name>")
                        print(f"   ì´ë¦„: {', '.join(MODEL_MAP.keys())} (gpt4=GPT-5.3-Codex)")
                        continue
                    
                    new_model = args[0].lower()
                    if new_model in MODEL_MAP:
                        CLOUD_MODEL_NAME = MODEL_MAP[new_model]
                        print(f"âœ… Cloud PM ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤: {CLOUD_MODEL_NAME}")
                        logger.info(f"ğŸ”„ Switched Cloud PM model to {CLOUD_MODEL_NAME}")
                    else:
                        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {new_model}")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(MODEL_MAP.keys())}")
                    continue

                else:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤: {cmd}")
                    print("   ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´: /new, /load, /model, /list, /current, /clear, /stats, /exit")
                    continue

            # â”€â”€ ì¼ë°˜ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            response = await process_request(user_input, router, worker, history, state, cache)
            
            if not response.startswith("[ERROR]"):
                last_msgs = history.get_full_history()[-1:]
                streamed = any(
                    (m.get("metadata") or {}).get("streamed")
                    for m in last_msgs
                )
                if not streamed:
                    print(f"\nğŸ¤– Assistant > {response}")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        # MCP ì—°ê²° ì¢…ë£Œ
        await global_mcp_manager.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
