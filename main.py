"""
Mac Mini M4 Hybrid AI Orchestration System â€” Enterprise Edition
================================================================
ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°: ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ 5ê³„ì¸µ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì¡°ìœ¨í•©ë‹ˆë‹¤.

Enterprise ê¸°ëŠ¥:
- ì˜ì†ì  ìƒíƒœ ê´€ë¦¬ + ì²´í¬í¬ì¸íŒ… (SQLite)
- ë™ì  í˜ë¥´ì†Œë‚˜ í† ê¸€ (YAML ê¸°ë°˜)
- ì ëŒ€ì  ê²€ì¦ (Devil's Advocate í† ë¡  ë£¨í”„)
- Human-in-the-loop (ì¸í„°ëŸ½íŠ¸ ê¸°ë°˜)

ë©€í‹° í”„ë¡œì íŠ¸(Contextualized Framework) ì§€ì›:
- /new <project>: ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
- /load <project>: í”„ë¡œì íŠ¸ ì „í™˜
- /list: í”„ë¡œì íŠ¸ ëª©ë¡
- /current: í˜„ì¬ ìƒíƒœ

Cloud PM ì „í™˜:
- /model <name>: í´ë¼ìš°ë“œ ëª¨ë¸ ë³€ê²½ (gemini, claude, gpt4)

Enterprise ëª…ë ¹ì–´:
- /checkpoint [label]: ìˆ˜ë™ ë§ˆì¼ìŠ¤í†¤ ì²´í¬í¬ì¸íŠ¸
- /rollback [step]: íŠ¹ì • ë‹¨ê³„ë¡œ ë¡¤ë°±
- /persona [id]: í˜ë¥´ì†Œë‚˜ ì „í™˜
- /debate: ë§ˆì§€ë§‰ ì‘ë‹µì— ì ëŒ€ì  ê²€ì¦ ì‹¤í–‰
- /approve: HITL ìŠ¹ì¸
- /reject: HITL ê±°ì ˆ

ì›Œí¬í”Œë¡œìš°:
1. ì‚¬ìš©ì ì…ë ¥ â†’ Router(DeepSeek-R1)ê°€ ë¶„ì„
2. LOCAL ë¼ìš°íŒ… â†’ Worker(Qwen 32B)ê°€ ì‹¤í–‰
3. CLOUD ë¼ìš°íŒ… ë˜ëŠ” [ESCALATE] â†’ Cloud PM(Gemini/Claude/GPT)ì´ ì²˜ë¦¬
4. ì ëŒ€ì  ê²€ì¦ (ì„ íƒì ) â†’ Devil's Advocate í† ë¡  ë£¨í”„
5. ê²°ê³¼ ë°˜í™˜ ë° ëŒ€í™” ê¸°ë¡ + ì²´í¬í¬ì¸íŠ¸ ì €ì¥

ì‹¤í–‰: python main.py
"""

import os
import sys
import asyncio
import logging
import yaml
from dotenv import load_dotenv
from openai import AsyncOpenAI

from agents.router import Router
from agents.worker import Worker
from core.state import AgentState, SessionStatus, CheckpointType
from core.checkpoint import CheckpointManager
from core.config_loader import ConfigLoader
from engine.persona import PersonaManager
from engine.adversarial import DebateLoop
from engine.hitl import HITLManager, WaitApproval
from utils.history_manager import HistoryManager, DEFAULT_HISTORY_DIR
from utils.mcp_client import global_mcp_manager
from utils.semantic_cache import SemanticCache

# â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

LITELLM_BASE_URL = os.getenv("LITELLM_BASE_URL", "http://localhost:4000")
CONTEXT_WINDOW = int(os.getenv("CONTEXT_WINDOW_SIZE", "20"))
HISTORY_DIR = os.getenv("HISTORY_DIR", "history")
CONFIG_FILE = "config.yaml"

# ê¸°ë³¸ í´ë¼ìš°ë“œ ëª¨ë¸
CLOUD_MODEL_NAME = "cloud-pm-gemini"

# ëª¨ë¸ ë§¤í•‘ (ë‹¨ì¶•ì–´ -> ì‹¤ì œ ëª¨ë¸ëª…)
MODEL_MAP = {
    "gemini": "cloud-pm-gemini",
    "claude": "cloud-pm-claude",
    "gpt4": "cloud-pm-gpt4",
}

# â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(name)-20s â”‚ %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("orchestrator")


def load_config() -> dict:
    """config.yaml íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(CONFIG_FILE):
        return {}
    try:
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except Exception as e:
        logger.error(f"âŒ Failed to load config.yaml: {e}")
        return {}


async def call_cloud_pm(
    task: str,
    context: list[dict] | None = None,
    base_url: str = LITELLM_BASE_URL,
    stream: bool = True,
    persona_manager: PersonaManager | None = None,
) -> str:
    """Cloud PM í˜¸ì¶œ (ë™ì  ëª¨ë¸ ì„ íƒ, ìŠ¤íŠ¸ë¦¬ë° ì§€ì›, í˜ë¥´ì†Œë‚˜ ì ìš©)"""
    client = AsyncOpenAI(base_url=base_url, api_key="not-needed")
    model_name = CLOUD_MODEL_NAME

    # í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if persona_manager and persona_manager.current:
        system_content = persona_manager.get_system_prompt()
    else:
        system_content = (
            "You are a senior project manager and architect with deep expertise "
            "in software design, complex reasoning, and strategic planning. "
            "Provide thorough, well-structured solutions."
        )

    messages = [{"role": "system", "content": system_content}]

    if context:
        messages.extend(context)

    messages.append({"role": "user", "content": task})

    try:
        if stream:
            response_stream = await client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=persona_manager.get_temperature() if persona_manager else 0.5,
                max_tokens=4096,
                stream=True,
            )
            chunks = []
            print("\nğŸ¤– Assistant > ", end="", flush=True)
            async for chunk in response_stream:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                chunks.append(content)
            print()
            return "".join(chunks) or "[Cloud PM returned empty response]"
        else:
            response = await client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=persona_manager.get_temperature() if persona_manager else 0.5,
                max_tokens=4096,
            )
            return response.choices[0].message.content or "[Cloud PM returned empty response]"
    except Exception as e:
        logger.error(f"âŒ Cloud PM í˜¸ì¶œ ì‹¤íŒ¨ ({model_name}): {e}")
        return f"[ERROR] Cloud PM ({model_name}) failed: {e}"


async def process_request(
    user_input: str,
    router: Router,
    worker: Worker,
    history: HistoryManager,
    state: AgentState,
    cache: SemanticCache | None = None,
    checkpoint_mgr: CheckpointManager | None = None,
    persona_mgr: PersonaManager | None = None,
    debate_loop: DebateLoop | None = None,
    hitl_mgr: HITLManager | None = None,
    enterprise_config: dict | None = None,
) -> str:
    """ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Enterprise Edition)"""
    ecfg = enterprise_config or {}
    history.add_message("user", user_input)
    state.increment_turn()
    state.increment_step()

    # â”€â”€ 0ë‹¨ê³„: Semantic Cache Lookup (Short-Circuit) â”€â”€â”€â”€â”€â”€
    if cache:
        cached = cache.get(user_input)
        if cached is not None:
            history.add_message(
                "assistant", cached,
                metadata={"handler": "semantic-cache", "cache_hit": True},
            )
            return cached

    # â”€â”€ 1ë‹¨ê³„: Sticky Routing ë˜ëŠ” Router í˜¸ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("=" * 60)

    if state.current_agent is not None:
        destination = state.current_agent
        reason = "Sticky Routing (ì´ì „ í„´ê³¼ ë™ì¼ ì—ì´ì „íŠ¸)"
        logger.info(f"ğŸ§­ [Sticky Route] Router ìŠ¤í‚µ â†’ {destination} | {reason}")
    else:
        logger.info("ğŸ§­ [Router] ì‘ì—… ë¶„ì„ ì¤‘...")
        routing = await router.route(user_input)
        destination = routing["destination"]
        reason = routing["reason"]
        state.current_agent = destination
        logger.info(f"ğŸ§­ [Router] ê²°ì •: {destination} | ì‚¬ìœ : {reason}")

    history.add_message(
        "system",
        f"[ROUTING] {destination}: {reason}",
        metadata={"type": "routing", "sticky": state.current_agent is not None},
    )

    # â”€â”€ ì²´í¬í¬ì¸íŠ¸: ë¼ìš°íŒ… ì§í›„ (TRANSACTION) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if checkpoint_mgr and ecfg.get("checkpoint_enabled", True):
        checkpoint_mgr.save_checkpoint(
            state, CheckpointType.TRANSACTION, label="post-routing"
        )

    # â”€â”€ 2ë‹¨ê³„: ë¼ìš°íŒ…ì— ë”°ë¼ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context = history.get_context()
    state.conversation_history = context

    if destination == "CLOUD":
        logger.info(f"â˜ï¸  [Cloud PM: {CLOUD_MODEL_NAME}] ê³ ë‚œë„ ì‘ì—… ì²˜ë¦¬ ì¤‘...")
        final_response = await call_cloud_pm(
            user_input, context=context, persona_manager=persona_mgr,
        )
        history.add_message(
            "assistant", final_response,
            metadata={
                "handler": CLOUD_MODEL_NAME,
                "reason": reason,
                "streamed": True,
                "persona": persona_mgr.current_id if persona_mgr else "default",
            },
        )
    else:
        logger.info("ğŸ”¨ [Worker] ì‘ì—… ì‹¤í–‰ ì¤‘...")
        result = await worker.execute(user_input, context=context)

        # Worker ì‹¤í–‰ ë©”íƒ€ ë¡œê¹…
        if result["helper_used"]:
            logger.info("  â””â”€ âœ… Helper í™œìš© ì™„ë£Œ")
        if result["helper_fallback"]:
            logger.info("  â””â”€ âš ï¸ Helper ì‹¤íŒ¨ â†’ Worker ì§ì ‘ ì²˜ë¦¬")

        v_passed = result.get("validation_passed", True)
        c_passed = result.get("critic_passed")
        if v_passed is False:
            logger.warning("  â””â”€ âŒ [Validator] ë¬¸ë²• ê²€ì¦ ì‹¤íŒ¨")
        if c_passed is False:
            logger.warning("  â””â”€ âŒ [Critic] ë¹„í‰ê°€ ê±°ì ˆ")

        if result["escalated"]:
            if c_passed is False:
                esc_reason = "critic-reject"
            elif v_passed is False:
                esc_reason = "validation-fail"
            else:
                esc_reason = "worker-escalation"

            logger.info(f"ğŸš¨ [Worker â†’ Cloud PM] ì—ìŠ¤ì»¬ë ˆì´ì…˜ ë°œìƒ! (ì‚¬ìœ : {esc_reason})")
            logger.info(f"â˜ï¸  [Cloud PM: {CLOUD_MODEL_NAME}] ë‚œì œ ì²˜ë¦¬ ì¤‘...")

            state.reset_routing()

            escalation_context = (
                f"ì´ì „ Workerì˜ ë¶„ì„:\n{result['response']}\n\n"
                f"ì›ë³¸ ìš”ì²­:\n{user_input}"
            )
            final_response = await call_cloud_pm(
                escalation_context, context=context, persona_manager=persona_mgr,
            )
            history.add_message(
                "assistant", final_response,
                metadata={
                    "handler": CLOUD_MODEL_NAME,
                    "reason": esc_reason,
                    "validation_passed": v_passed,
                    "critic_passed": c_passed,
                    "worker_response": result["response"][:500],
                },
            )
        else:
            final_response = result["response"]
            history.add_message(
                "assistant", final_response,
                metadata={
                    "handler": "local-worker",
                    "helper_used": result["helper_used"],
                    "helper_fallback": result["helper_fallback"],
                    "validation_passed": v_passed,
                    "critic_passed": c_passed,
                },
            )

    # â”€â”€ 3ë‹¨ê³„: ì ëŒ€ì  ê²€ì¦ (ì„ íƒì ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (
        debate_loop
        and ecfg.get("debate_enabled", False)
        and ecfg.get("debate_auto_trigger_on_cloud", False)
        and destination == "CLOUD"
    ):
        logger.info("âš”ï¸ Auto-triggering adversarial debate...")
        debate_result = await debate_loop.run(
            proposal=final_response,
            task=user_input,
            max_rounds=ecfg.get("debate_max_rounds", 3),
            approval_threshold=ecfg.get("debate_approval_threshold", 7.0),
        )
        if debate_result.approved:
            final_response = debate_result.final_proposal
            logger.info(
                f"âš”ï¸ Debate approved after {debate_result.total_rounds} rounds"
            )
        elif debate_result.escalated:
            logger.warning("ğŸš¨ Debate escalated â†’ HITL required")
            if hitl_mgr:
                await hitl_mgr.suspend(
                    state, "Adversarial debate escalation",
                    {"debate_report": debate_result.report},
                )

    # â”€â”€ ì²´í¬í¬ì¸íŠ¸: ì‘ì—… ì™„ë£Œ (MILESTONE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if checkpoint_mgr and ecfg.get("checkpoint_enabled", True):
        checkpoint_mgr.save_checkpoint(
            state, CheckpointType.MILESTONE, label="task-complete"
        )

    logger.info("=" * 60)

    # ì„±ê³µì ì¸ ì‘ë‹µì„ ìºì‹œì— ì €ì¥
    if cache and not final_response.startswith("[ERROR]"):
        cache.put(user_input, final_response)

    return final_response


def print_banner() -> None:
    """ì‹œì‘ ë°°ë„ˆ ì¶œë ¥"""
    banner = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       ğŸ¤– Mac Mini M4 Hybrid AI â€” Enterprise Edition        â•‘
â•‘       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â•‘
â•‘   Commands:                                                  â•‘
â•‘    /new <project>    : ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± ë° ì „í™˜               â•‘
â•‘    /load <project>   : ê¸°ì¡´ í”„ë¡œì íŠ¸ ë¡œë“œ                     â•‘
â•‘    /model <name>     : Cloud PM ëª¨ë¸ ë³€ê²½ (gemini/claude...) â•‘
â•‘                        (Current: {CLOUD_MODEL_NAME})         â•‘
â•‘    /list             : í”„ë¡œì íŠ¸ ëª©ë¡ í™•ì¸                     â•‘
â•‘    /current          : í˜„ì¬ ìƒíƒœ í™•ì¸                         â•‘
â•‘    /clear            : ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”                       â•‘
â•‘    /stats            : ëŒ€í™” í†µê³„                              â•‘
â•‘   Enterprise:                                                â•‘
â•‘    /persona <id>     : í˜ë¥´ì†Œë‚˜ ì „í™˜                          â•‘
â•‘    /checkpoint [lbl] : ìˆ˜ë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥                    â•‘
â•‘    /rollback [step]  : ì²´í¬í¬ì¸íŠ¸ ë¡¤ë°±                        â•‘
â•‘    /debate           : ë§ˆì§€ë§‰ ì‘ë‹µì— ì ëŒ€ì  ê²€ì¦               â•‘
â•‘    /approve | /reject: HITL ìŠ¹ì¸/ê±°ì ˆ                          â•‘
â•‘    /exit             : ì¢…ë£Œ                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)


def switch_project(project_name: str) -> HistoryManager:
    """í”„ë¡œì íŠ¸ ì„¸ì…˜ ì „í™˜"""
    history = HistoryManager(
        project_name=project_name,
        base_dir=HISTORY_DIR,
        context_window=CONTEXT_WINDOW,
    )
    print(f"\nğŸ“‚ í”„ë¡œì íŠ¸ ì „í™˜ ì™„ë£Œ: [{project_name}]")
    logger.info(f"ğŸ“‚ Active Project switched to: {project_name}")
    return history


async def main() -> None:
    """ë©”ì¸ ë£¨í”„ (async)"""
    global CLOUD_MODEL_NAME
    print_banner()

    # â”€â”€ ì´ˆê¸° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not os.path.exists(HISTORY_DIR):
        os.makedirs(HISTORY_DIR)

    # Config ë¡œë“œ
    config = load_config()
    mcp_config = config.get("mcp_servers", {})

    # Enterprise ì„¤ì • ë¡œë“œ
    enterprise_config_loader = ConfigLoader()
    enterprise_config = enterprise_config_loader.base.get("system", {})

    router = Router(base_url=LITELLM_BASE_URL)
    worker = Worker(base_url=LITELLM_BASE_URL)

    # MCP ë„êµ¬ ì´ˆê¸°í™”
    if mcp_config:
        await worker.initialize_mcp_tools(mcp_config)

    # ê¸°ë³¸ í”„ë¡œì íŠ¸ë¡œ ì‹œì‘
    history = HistoryManager(
        project_name="default",
        base_dir=HISTORY_DIR,
        context_window=CONTEXT_WINDOW,
    )

    # Enterprise ì¸í”„ë¼ ì´ˆê¸°í™”
    state = AgentState()
    cache = SemanticCache()
    checkpoint_mgr = CheckpointManager(db_dir=HISTORY_DIR)
    persona_mgr = PersonaManager(config_loader=enterprise_config_loader)
    debate_loop = DebateLoop(
        persona_manager=persona_mgr,
        base_url=LITELLM_BASE_URL,
    )
    hitl_mgr = HITLManager(checkpoint_manager=checkpoint_mgr)

    # ë§ˆì§€ë§‰ ì‘ë‹µ ì¶”ì  (ì ëŒ€ì  ê²€ì¦ìš©)
    last_response: str = ""

    logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (Enterprise Edition)")
    logger.info(f"ğŸ“¡ LiteLLM Proxy: {LITELLM_BASE_URL}")
    logger.info(f"ğŸ­ Active Persona: {persona_mgr.current_id}")

    # â”€â”€ ì¸í„°ë™í‹°ë¸Œ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        while True:
            try:
                persona_label = persona_mgr.current_id
                prompt_text = (
                    f"\n[{history.project_name} | "
                    f"{CLOUD_MODEL_NAME.split('-')[-1]} | "
                    f"ğŸ­ {persona_label}] ğŸ§‘ You > "
                )
                user_input = await asyncio.to_thread(input, prompt_text)
                user_input = user_input.strip()
            except EOFError:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not user_input:
                continue

            # â”€â”€ CLI ëª…ë ¹ì–´ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if user_input.startswith("/"):
                parts = user_input.split()
                cmd = parts[0].lower()
                args = parts[1:] if len(parts) > 1 else []

                if cmd in ("/exit", "/quit"):
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                elif cmd == "/clear":
                    history.clear()
                    state = AgentState()
                    continue

                elif cmd == "/stats":
                    stats = history.get_stats()
                    print(f"\nğŸ“Š í†µê³„ ({stats['project']}):")
                    print(f"   ë©”ì‹œì§€ ìˆ˜: {stats['total_messages']}")
                    print(f"   íŒŒì¼ ê²½ë¡œ: {stats['file_path']}")
                    full = history.get_full_history()
                    handlers: dict[str, int] = {}
                    for msg in full:
                        h = (msg.get("metadata") or {}).get("handler", "")
                        if h:
                            handlers[h] = handlers.get(h, 0) + 1
                    if handlers:
                        print("   í•¸ë“¤ëŸ¬ ë¶„í¬:")
                        for h, cnt in sorted(handlers.items(), key=lambda x: -x[1]):
                            print(f"     {h}: {cnt}íšŒ")
                    continue

                elif cmd == "/list":
                    projects = HistoryManager.list_projects(HISTORY_DIR)
                    print("\nğŸ“‚ ì €ì¥ëœ í”„ë¡œì íŠ¸ ëª©ë¡:")
                    for p in projects:
                        print(f"   - {p}")
                    continue

                elif cmd == "/current":
                    print(f"\nğŸ” í˜„ì¬ ìƒíƒœ:")
                    print(f"   í”„ë¡œì íŠ¸: {history.project_name}")
                    print(f"   Cloud PM: {CLOUD_MODEL_NAME}")
                    print(f"   í˜ë¥´ì†Œë‚˜: {persona_mgr.current_id}")
                    print(f"   ì„¸ì…˜ ID: {state.session_id[:8]}...")
                    print(f"   ë‹¨ê³„: {state.step}")
                    print(f"   ìƒíƒœ: {state.status.value}")
                    continue

                elif cmd == "/new":
                    if not args:
                        print("âš ï¸ ì‚¬ìš©ë²•: /new <project_name>")
                        continue
                    history = switch_project(args[0])
                    state = AgentState()
                    continue

                elif cmd == "/load":
                    if not args:
                        print("âš ï¸ ì‚¬ìš©ë²•: /load <project_name>")
                        continue
                    history = switch_project(args[0])
                    state = AgentState()
                    continue

                elif cmd == "/model":
                    if not args:
                        print(f"âš ï¸ ì‚¬ìš©ë²•: /model <name>")
                        print(f"   ì´ë¦„: {', '.join(MODEL_MAP.keys())} (gpt4=GPT-5.3-Codex)")
                        continue

                    new_model = args[0].lower()
                    if new_model in MODEL_MAP:
                        CLOUD_MODEL_NAME = MODEL_MAP[new_model]
                        print(f"âœ… Cloud PM ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤: {CLOUD_MODEL_NAME}")
                        logger.info(f"ğŸ”„ Switched Cloud PM model to {CLOUD_MODEL_NAME}")
                    else:
                        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {new_model}")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(MODEL_MAP.keys())}")
                    continue

                # â”€â”€ Enterprise ëª…ë ¹ì–´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                elif cmd == "/persona":
                    if not args:
                        available = persona_mgr.available_personas()
                        print(f"\nğŸ­ í˜„ì¬ í˜ë¥´ì†Œë‚˜: {persona_mgr.current_id}")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(available)}")
                        continue
                    try:
                        new_persona = persona_mgr.switch_persona(
                            args[0], reason="Manual switch via CLI"
                        )
                        print(
                            f"ğŸ­ í˜ë¥´ì†Œë‚˜ ì „í™˜: {new_persona.display_name} "
                            f"(temp={new_persona.temperature})"
                        )
                    except FileNotFoundError:
                        print(f"âš ï¸ í˜ë¥´ì†Œë‚˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args[0]}")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(persona_mgr.available_personas())}")
                    continue

                elif cmd == "/checkpoint":
                    label = " ".join(args) if args else "manual"
                    checkpoint_mgr.save_checkpoint(
                        state, CheckpointType.MILESTONE, label=label
                    )
                    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: step={state.step}, label='{label}'")
                    continue

                elif cmd == "/rollback":
                    if not args:
                        cps = checkpoint_mgr.list_checkpoints(state.session_id)
                        if cps:
                            print("\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡:")
                            for cp in cps:
                                print(
                                    f"   step={cp['step']} | "
                                    f"type={cp['type']} | "
                                    f"label='{cp['label']}' | "
                                    f"{cp['created_at']}"
                                )
                        else:
                            print("âš ï¸ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    try:
                        step = int(args[0])
                        restored = checkpoint_mgr.rollback(state.session_id, step)
                        if restored:
                            state = restored
                            print(f"âª ë¡¤ë°± ì™„ë£Œ: step={step}")
                        else:
                            print(f"âš ï¸ step={step}ì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    except ValueError:
                        print("âš ï¸ ì‚¬ìš©ë²•: /rollback <step_number>")
                    continue

                elif cmd == "/debate":
                    if not last_response:
                        print("âš ï¸ ê²€ì¦í•  ë§ˆì§€ë§‰ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    print("âš”ï¸ ì ëŒ€ì  ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                    debate_result = await debate_loop.run(
                        proposal=last_response,
                        task="(ë§ˆì§€ë§‰ ì‘ë‹µ ê²€ì¦)",
                        max_rounds=enterprise_config.get("debate_max_rounds", 3),
                        approval_threshold=enterprise_config.get(
                            "debate_approval_threshold", 7.0
                        ),
                    )
                    print(f"\nâš”ï¸ ê²€ì¦ ê²°ê³¼:")
                    print(f"   ìŠ¹ì¸: {'âœ…' if debate_result.approved else 'âŒ'}")
                    print(f"   ë¼ìš´ë“œ: {debate_result.total_rounds}")
                    if debate_result.report:
                        print(f"\n{debate_result.report}")
                    if debate_result.approved and debate_result.final_proposal != last_response:
                        last_response = debate_result.final_proposal
                        print(f"\nğŸ¤– ìˆ˜ì •ëœ ì‘ë‹µ > {last_response}")
                    continue

                elif cmd == "/approve":
                    restored = await hitl_mgr.resume(
                        state.session_id, action="approve"
                    )
                    if restored:
                        state = restored
                        print("âœ… ìŠ¹ì¸ ì™„ë£Œ. ì—ì´ì „íŠ¸ê°€ ì¬ê°œë©ë‹ˆë‹¤.")
                    else:
                        print("âš ï¸ ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                elif cmd == "/reject":
                    result = await hitl_mgr.resume(
                        state.session_id, action="reject"
                    )
                    print("âŒ ê±°ì ˆ ì™„ë£Œ.")
                    continue

                else:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤: {cmd}")
                    print("   ê¸°ë³¸: /new, /load, /model, /list, /current, /clear, /stats, /exit")
                    print("   Enterprise: /persona, /checkpoint, /rollback, /debate, /approve, /reject")
                    continue

            # â”€â”€ HITL ì¤‘ë‹¨ ìƒíƒœ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if state.status == SessionStatus.SUSPENDED:
                pending = hitl_mgr.get_pending(state.session_id)
                if pending:
                    print(
                        f"â¸ï¸ ì—ì´ì „íŠ¸ê°€ ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤: {pending.get('reason', '')}"
                    )
                    print("   /approve ë˜ëŠ” /reject ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                    continue

            # â”€â”€ ì¼ë°˜ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            try:
                response = await process_request(
                    user_input, router, worker, history, state, cache,
                    checkpoint_mgr=checkpoint_mgr,
                    persona_mgr=persona_mgr,
                    debate_loop=debate_loop,
                    hitl_mgr=hitl_mgr,
                    enterprise_config=enterprise_config,
                )
                last_response = response

                if not response.startswith("[ERROR]"):
                    last_msgs = history.get_full_history()[-1:]
                    streamed = any(
                        (m.get("metadata") or {}).get("streamed")
                        for m in last_msgs
                    )
                    if not streamed:
                        print(f"\nğŸ¤– Assistant > {response}")

            except WaitApproval as e:
                # HITL ì¸í„°ëŸ½íŠ¸ ì²˜ë¦¬
                await hitl_mgr.suspend(
                    state, e.reason,
                    {"function": e.function_name, "args": e.function_args},
                )
                print(
                    f"\nâ¸ï¸ ì—ì´ì „íŠ¸ê°€ ìŠ¹ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤: {e.reason}"
                )
                print("   /approve ë˜ëŠ” /reject ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        # MCP ì—°ê²° ì¢…ë£Œ
        await global_mcp_manager.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
